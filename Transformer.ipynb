{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
    "\n",
    "They suggest at least one million words of text.\n",
    "\n",
    "https://stackabuse.com/gpt-style-text-generation-in-python-with-tensorflowkeras/\n",
    "\n",
    "We will work on full sentences. Let's use the bigger dataset and simply remove the longest sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import glob\n",
    "import textwrap\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.utils as ku\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nlp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 MB of Polish novels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reading files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing chapter names\n",
    "\n",
    "def remove_chapter_names(input_string, regex_string):\n",
    "  a1 = input_string\n",
    "  a2 = re.sub(rf'{regex_string}', '', a1)\n",
    "  return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 285068 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for file in glob.glob(\"30 MB noweli/*\"):\n",
    "\n",
    "    #read the file\n",
    "    myfile = open(file,\"r\")\n",
    "    text = myfile.read()\n",
    "    myfile.close()\n",
    "\n",
    "    #clean chapter names\n",
    "    text = remove_chapter_names(text, 'ROZDZIAŁ[^\\n]+')\n",
    "    text = remove_chapter_names(text, 'Rozdział[^\\n]+')\n",
    "\n",
    "    #lower\n",
    "    text = text.lower()\n",
    "\n",
    "    #split to sentences\n",
    "    text = sent_tokenize(text)\n",
    "    #print(\"file \", file, \" generated \", len(text), \" sentences\")\n",
    "    \n",
    "    sentences.extend(text)\n",
    "print(\"We have\", len(sentences), \"sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text consists of 4497449 words.\n"
     ]
    }
   ],
   "source": [
    "continuous_corpus = \" \".join(sentences)\n",
    "print(\"Full text consists of\", len(continuous_corpus.replace('\\n', ' ').split(' ')), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agada i przypowieść\\n\\njeśli chcesz poznać stwórcę świata, czytaj agadę.',\n",
       " 'przez nią zrozumiesz istotę boga, oby był błogosławiony.',\n",
       " 'dzięki niej będziesz wiedział, jak się zachować i kroczyć jego drogami.',\n",
       " 'nie traktuj lekko przypowieści.',\n",
       " 'z małą, groszową świeczką można czasem znaleźć monetę albo cenną perłę.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sentences lengths analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are of length 1 to 353\n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "for sentence in sentences:\n",
    "  lens.append(len(sentence.replace('\\n', ' ').split(' ')))\n",
    "\n",
    "print(\"Sentences are of length\", min(lens), \"to\", max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles:\n",
      "0.15 is 5 \n",
      "0.5 is 13 \n",
      "0.8 is 24 \n",
      "0.9 is 32 \n",
      "0.95 is 40\n",
      "Let's remove the sentences longer than 40.\n"
     ]
    }
   ],
   "source": [
    "#quantiles - 90% of sequences consists of at most 32 words, at most 15% is of length 5 or less\n",
    "lens.sort()\n",
    "print(\"Quantiles:\\n0.15 is\", lens[int(0.15*len(lens))],\n",
    " \"\\n0.5 is\", lens[int(0.5*len(lens))], \n",
    " \"\\n0.8 is\", lens[int(0.8*len(lens))], \n",
    " \"\\n0.9 is\", lens[int(0.9*len(lens))],\n",
    " \"\\n0.95 is\", lens[int(0.95*len(lens))])\n",
    "print(\"Let's remove the sentences longer than 40.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing long sentences\n",
    "#lowering the letters\n",
    "#removing new line signs\n",
    "\n",
    "sentences_short = []\n",
    "for sentence in sentences:\n",
    "  if not len(sentence.replace('\\n', ' ').split(' ')) > 40:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('\\n', ' ')\n",
    "    sentence = sentence.replace('—', '-')\n",
    "    sentences_short.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short sentences are of length 1 to 40\n"
     ]
    }
   ],
   "source": [
    "lens2 = []\n",
    "for sentence in sentences_short:\n",
    "  lens2.append(len(sentence.replace('\\n', ' ').split(' ')))\n",
    "\n",
    "print(\"Short sentences are of length\", min(lens2), \"to\", max(lens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wreście, nie pozdrowiwszy ich po chrześciańsku, co ich uderzyło obu, stary zawołał.',\n",
       " 'a spotka, bądź tego pewny!',\n",
       " 'ale nawet nie czekając tego pociągu, byłbym mógł, ubrawszy się spiesznie, jechać jeszcze tego wieczora, gdyby rodzice mi pozwolili.',\n",
       " 'był nadzwyczaj nieostrożny, a więc prawdopodobnie młody.',\n",
       " 'i w pierwszej chwili uniesienia i wdzięczności padł jej do nóg.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(sentences_short)\n",
    "sentences_short[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenization. No punctuation included**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words: 222657\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Tokenizer on the Corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_short)\n",
    "\n",
    "# Vocabulary count of the corpus\n",
    "total_words = len(tokenizer.word_index)\n",
    "\n",
    "print(\"Total Unique Words:\", total_words)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the text into embeddings\n",
    "input_sequences = []\n",
    "for sentence in sentences_short:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    input_sequences.append(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Padding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(lens2)\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=maxlen+1, padding='pre'))  #maxlen +1\n",
    "\n",
    "# predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "# #label = ku.to_categorical(label1, num_classes=total_words+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictors.shape, label.shape      #sample of length 40 was cut by one (because it is a label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'i',\n",
       " 2: 'się',\n",
       " 3: 'w',\n",
       " 4: 'nie',\n",
       " 5: 'na',\n",
       " 6: 'z',\n",
       " 7: 'do',\n",
       " 8: 'to',\n",
       " 9: 'że',\n",
       " 10: 'a',\n",
       " 11: 'o',\n",
       " 12: 'ale',\n",
       " 13: 'jak',\n",
       " 14: 'co',\n",
       " 15: 'tak',\n",
       " 16: 'za',\n",
       " 17: 'po',\n",
       " 18: 'jest',\n",
       " 19: 'go',\n",
       " 20: 'od',\n",
       " 21: 'już',\n",
       " 22: 'mu',\n",
       " 23: 'jego',\n",
       " 24: 'było',\n",
       " 25: 'mnie',\n",
       " 26: 'tego',\n",
       " 27: 'jej',\n",
       " 28: 'tylko',\n",
       " 29: 'mi',\n",
       " 30: 'był',\n",
       " 31: '–',\n",
       " 32: 'dla',\n",
       " 33: 'sobie',\n",
       " 34: 'jeszcze',\n",
       " 35: 'tym',\n",
       " 36: 'ich',\n",
       " 37: 'ja',\n",
       " 38: 'przez',\n",
       " 39: 'bo',\n",
       " 40: 'ze',\n",
       " 41: 'gdy',\n",
       " 42: 'który',\n",
       " 43: 'może',\n",
       " 44: 'ten',\n",
       " 45: 'aby',\n",
       " 46: 'czy',\n",
       " 47: 'pan',\n",
       " 48: 'tu',\n",
       " 49: 'nim',\n",
       " 50: 'ją',\n",
       " 51: 'pod',\n",
       " 52: 'rzekł',\n",
       " 53: 'by',\n",
       " 54: 'tej',\n",
       " 55: 'nawet',\n",
       " 56: 'ci',\n",
       " 57: 'on',\n",
       " 58: 'przed',\n",
       " 59: 'była',\n",
       " 60: 'które',\n",
       " 61: 'tam',\n",
       " 62: 'być',\n",
       " 63: 'przy',\n",
       " 64: 'wszystko',\n",
       " 65: 'iż',\n",
       " 66: 'ma',\n",
       " 67: 'teraz',\n",
       " 68: 'sam',\n",
       " 69: 'nic',\n",
       " 70: 'więc',\n",
       " 71: 'miał',\n",
       " 72: 'nad',\n",
       " 73: 'będzie',\n",
       " 74: 'kiedy',\n",
       " 75: 'u',\n",
       " 76: 'też',\n",
       " 77: 'bez',\n",
       " 78: 'bardzo',\n",
       " 79: 'ani',\n",
       " 80: 'jako',\n",
       " 81: 'lecz',\n",
       " 82: 'tych',\n",
       " 83: 'niego',\n",
       " 84: 'siebie',\n",
       " 85: 'nas',\n",
       " 86: 'gdzie',\n",
       " 87: 'zaś',\n",
       " 88: 'jednak',\n",
       " 89: 'są',\n",
       " 90: 'chwili',\n",
       " 91: 'je',\n",
       " 92: 'jakby',\n",
       " 93: 'nich',\n",
       " 94: 'oczy',\n",
       " 95: 'jeśli',\n",
       " 96: 'mógł',\n",
       " 97: 'niż',\n",
       " 98: 'im',\n",
       " 99: 'te',\n",
       " 100: 'niej',\n",
       " 101: 'pani',\n",
       " 102: 'ku',\n",
       " 103: 'gdyby',\n",
       " 104: 'kto',\n",
       " 105: 'trzeba',\n",
       " 106: 'żeby',\n",
       " 107: 'ta',\n",
       " 108: 'aż',\n",
       " 109: 'lub',\n",
       " 110: 'która',\n",
       " 111: 'coś',\n",
       " 112: 'ty',\n",
       " 113: 'więcej',\n",
       " 114: 'zawsze',\n",
       " 115: 'których',\n",
       " 116: 'właśnie',\n",
       " 117: 'wszystkie',\n",
       " 118: 'ona',\n",
       " 119: 'pana',\n",
       " 120: 'były',\n",
       " 121: 'cię',\n",
       " 122: 'ludzi',\n",
       " 123: 'raz',\n",
       " 124: 'tem',\n",
       " 125: 'wszystkich',\n",
       " 126: 'można',\n",
       " 127: 'między',\n",
       " 128: 'którego',\n",
       " 129: 'nam',\n",
       " 130: 'nigdy',\n",
       " 131: 'potem',\n",
       " 132: 'dobrze',\n",
       " 133: 'sobą',\n",
       " 134: 'panie',\n",
       " 135: 'król',\n",
       " 136: '\\u2009',\n",
       " 137: 'niech',\n",
       " 138: 'nią',\n",
       " 139: 'jeden',\n",
       " 140: 'którzy',\n",
       " 141: 'we',\n",
       " 142: 'mam',\n",
       " 143: 'wszyscy',\n",
       " 144: 'bóg',\n",
       " 145: 'życia',\n",
       " 146: 'której',\n",
       " 147: 'tę',\n",
       " 148: 'mówił',\n",
       " 149: 'życie',\n",
       " 150: 'albo',\n",
       " 151: 'mój',\n",
       " 152: 'jestem',\n",
       " 153: 'zaraz',\n",
       " 154: 'człowiek',\n",
       " 155: 'chciał',\n",
       " 156: 'wiem',\n",
       " 157: 'ludzie',\n",
       " 158: 'którym',\n",
       " 159: 'rzeczy',\n",
       " 160: 'czasu',\n",
       " 161: 'dalej',\n",
       " 162: 'domu',\n",
       " 163: 'czas',\n",
       " 164: 'de',\n",
       " 165: 'prawie',\n",
       " 166: 'którą',\n",
       " 167: 'ręce',\n",
       " 168: 'coraz',\n",
       " 169: 'znowu',\n",
       " 170: 'książę',\n",
       " 171: 'cóż',\n",
       " 172: 'myśli',\n",
       " 173: 'słowa',\n",
       " 174: 'dziś',\n",
       " 175: 'cały',\n",
       " 176: 'dzień',\n",
       " 177: 'nagle',\n",
       " 178: 'czym',\n",
       " 179: 'sposób',\n",
       " 180: 'także',\n",
       " 181: 'swego',\n",
       " 182: 'jakie',\n",
       " 183: 'dnia',\n",
       " 184: 'nikt',\n",
       " 185: 'choć',\n",
       " 186: 'don',\n",
       " 187: 'dopiero',\n",
       " 188: 'gdyż',\n",
       " 189: 'wreszcie',\n",
       " 190: 'mimo',\n",
       " 191: 'króla',\n",
       " 192: 'oto',\n",
       " 193: 'głowę',\n",
       " 194: 'miała',\n",
       " 195: 'począł',\n",
       " 196: 'jeno',\n",
       " 197: 'byli',\n",
       " 198: 'myśl',\n",
       " 199: 'nimi',\n",
       " 200: 'tyle',\n",
       " 201: 'kilka',\n",
       " 202: 'twarz',\n",
       " 203: 'dwa',\n",
       " 204: 'moje',\n",
       " 205: 'czego',\n",
       " 206: 'was',\n",
       " 207: 'bowiem',\n",
       " 208: 'razem',\n",
       " 209: 'dlatego',\n",
       " 210: 'swoje',\n",
       " 211: 'jakiś',\n",
       " 212: '…',\n",
       " 213: 'samo',\n",
       " 214: 'my',\n",
       " 215: 'bardziej',\n",
       " 216: 'jeżeli',\n",
       " 217: '”',\n",
       " 218: 'ciebie',\n",
       " 219: 'zupełnie',\n",
       " 220: 'ziemi',\n",
       " 221: 'chwilę',\n",
       " 222: 'zawołał',\n",
       " 223: 'wiele',\n",
       " 224: 'swoich',\n",
       " 225: 'musi',\n",
       " 226: 'wśród',\n",
       " 227: 'strony',\n",
       " 228: 'wówczas',\n",
       " 229: 'człowieka',\n",
       " 230: 'sama',\n",
       " 231: 'zagłoba',\n",
       " 232: 'całą',\n",
       " 233: 'pierwszy',\n",
       " 234: 'rzecz',\n",
       " 235: 'taki',\n",
       " 236: 'masz',\n",
       " 237: 'każdy',\n",
       " 238: 'jedynie',\n",
       " 239: 'tymczasem',\n",
       " 240: 'odpowiedział',\n",
       " 241: 'zresztą',\n",
       " 242: 'rękę',\n",
       " 243: 'ile',\n",
       " 244: 'moja',\n",
       " 245: 'prawo',\n",
       " 246: 'no',\n",
       " 247: 'oni',\n",
       " 248: 'prawa',\n",
       " 249: 'lepiej',\n",
       " 250: 'głos',\n",
       " 251: 'stary',\n",
       " 252: 'wtedy',\n",
       " 253: 'będę',\n",
       " 254: 'swoją',\n",
       " 255: 'mogę',\n",
       " 256: 'stał',\n",
       " 257: 'dni',\n",
       " 258: 'długo',\n",
       " 259: 'drzwi',\n",
       " 260: 'temu',\n",
       " 261: 'powiedział',\n",
       " 262: 'takie',\n",
       " 263: 'innych',\n",
       " 264: 'jednego',\n",
       " 265: 'duszy',\n",
       " 266: 'dość',\n",
       " 267: 'paweł',\n",
       " 268: 'mogła',\n",
       " 269: 'śmierć',\n",
       " 270: 'świat',\n",
       " 271: 'niby',\n",
       " 272: 'mieć',\n",
       " 273: 'julian',\n",
       " 274: 'świecie',\n",
       " 275: 'wam',\n",
       " 276: 'mówić',\n",
       " 277: 'sancho',\n",
       " 278: 'całe',\n",
       " 279: 'rabi',\n",
       " 280: 'znów',\n",
       " 281: 'stało',\n",
       " 282: 'wiedział',\n",
       " 283: 'boga',\n",
       " 284: 'mieli',\n",
       " 285: 'ów',\n",
       " 286: 'wcale',\n",
       " 287: 'musiał',\n",
       " 288: 'twarzy',\n",
       " 289: 'głowy',\n",
       " 290: 'jaki',\n",
       " 291: 'nieco',\n",
       " 292: 'widział',\n",
       " 293: 'skoro',\n",
       " 294: 'śmierci',\n",
       " 295: 'tedy',\n",
       " 296: 'lat',\n",
       " 297: 'trochę',\n",
       " 298: 'mają',\n",
       " 299: 'swej',\n",
       " 300: 'świata',\n",
       " 301: 'serca',\n",
       " 302: 'natychmiast',\n",
       " 303: 'swoim',\n",
       " 304: 'przeciw',\n",
       " 305: 'serce',\n",
       " 306: 'chce',\n",
       " 307: 'prawda',\n",
       " 308: 'jaką',\n",
       " 309: 'jakieś',\n",
       " 310: 'dał',\n",
       " 311: 'oczyma',\n",
       " 312: 'dzieci',\n",
       " 313: 'drugi',\n",
       " 314: 'powiedzieć',\n",
       " 315: 'odparł',\n",
       " 316: 'później',\n",
       " 317: 'powiada',\n",
       " 318: 'często',\n",
       " 319: 'zaczął',\n",
       " 320: 'kmicic',\n",
       " 321: 'nocy',\n",
       " 322: 'trzy',\n",
       " 323: 'chyba',\n",
       " 324: 'razy',\n",
       " 325: 'panu',\n",
       " 326: 'inaczej',\n",
       " 327: 'przecie',\n",
       " 328: 'wszystkim',\n",
       " 329: 'widać',\n",
       " 330: 'choćby',\n",
       " 331: 'wobec',\n",
       " 332: 'miłość',\n",
       " 333: 'ksiądz',\n",
       " 334: 'będą',\n",
       " 335: 'takim',\n",
       " 336: 'mniej',\n",
       " 337: 'wie',\n",
       " 338: 'razie',\n",
       " 339: 'kichot',\n",
       " 340: 'mną',\n",
       " 341: 'jedno',\n",
       " 342: 'czasem',\n",
       " 343: 'rzekła',\n",
       " 344: 'ponieważ',\n",
       " 345: 'dać',\n",
       " 346: 'noc',\n",
       " 347: 'ciągle',\n",
       " 348: 'głosem',\n",
       " 349: 'drogę',\n",
       " 350: 'zbyt',\n",
       " 351: 'samego',\n",
       " 352: 'miejsce',\n",
       " 353: 'miejsca',\n",
       " 354: 'ojca',\n",
       " 355: 'księcia',\n",
       " 356: 'inne',\n",
       " 357: 'drogi',\n",
       " 358: 'wy',\n",
       " 359: 'ni',\n",
       " 360: 'zapytał',\n",
       " 361: 'głową',\n",
       " 362: 'boże',\n",
       " 363: 'mało',\n",
       " 364: 'dwóch',\n",
       " 365: 'ojciec',\n",
       " 366: 'odrzekł',\n",
       " 367: 'konia',\n",
       " 368: 'kazał',\n",
       " 369: 'wielki',\n",
       " 370: 'również',\n",
       " 371: 'raczej',\n",
       " 372: 'wołodyjowski',\n",
       " 373: 'siły',\n",
       " 374: 'konie',\n",
       " 375: 'naprzód',\n",
       " 376: 'stanie',\n",
       " 377: 'woli',\n",
       " 378: 'dwie',\n",
       " 379: 'zdawało',\n",
       " 380: 'drodze',\n",
       " 381: 'bym',\n",
       " 382: 'jednej',\n",
       " 383: 'jesteś',\n",
       " 384: 'chwila',\n",
       " 385: 'samym',\n",
       " 386: 'ktoś',\n",
       " 387: 'kilku',\n",
       " 388: 'koniec',\n",
       " 389: 'spytał',\n",
       " 390: 'jednym',\n",
       " 391: 'chociaż',\n",
       " 392: 'czuł',\n",
       " 393: 'swą',\n",
       " 394: 'chcę',\n",
       " 395: 'cała',\n",
       " 396: 'życiu',\n",
       " 397: 'jedna',\n",
       " 398: 'widok',\n",
       " 399: 'swojej',\n",
       " 400: 'takich',\n",
       " 401: 'ziemię',\n",
       " 402: 'obok',\n",
       " 403: 'całej',\n",
       " 404: 'jutro',\n",
       " 405: 'miejscu',\n",
       " 406: 'dodał',\n",
       " 407: 'mogło',\n",
       " 408: 'słowo',\n",
       " 409: 'dotąd',\n",
       " 410: 'każdym',\n",
       " 411: 'innego',\n",
       " 412: 'mogli',\n",
       " 413: 'list',\n",
       " 414: 'miłości',\n",
       " 415: 'inni',\n",
       " 416: 'swój',\n",
       " 417: 'mogą',\n",
       " 418: 'niemal',\n",
       " 419: 'takiego',\n",
       " 420: 'czasie',\n",
       " 421: 'oczach',\n",
       " 422: 'góry',\n",
       " 423: 'nasze',\n",
       " 424: 'myślał',\n",
       " 425: 'poza',\n",
       " 426: 'wiesz',\n",
       " 427: 'bądź',\n",
       " 428: 'należy',\n",
       " 429: 'taką',\n",
       " 430: 'przecież',\n",
       " 431: 'dosyć',\n",
       " 432: 'pokoju',\n",
       " 433: 'jakże',\n",
       " 434: 'iść',\n",
       " 435: 'wielu',\n",
       " 436: 'ledwie',\n",
       " 437: 'mości',\n",
       " 438: 'wedle',\n",
       " 439: 'tą',\n",
       " 440: 'wielką',\n",
       " 441: 'ręką',\n",
       " 442: 'krew',\n",
       " 443: 'widzę',\n",
       " 444: 'krwi',\n",
       " 445: 'biskup',\n",
       " 446: 'wielkie',\n",
       " 447: 'muszę',\n",
       " 448: 'piersi',\n",
       " 449: 'nami',\n",
       " 450: 'jedną',\n",
       " 451: 'nogi',\n",
       " 452: 'zdaje',\n",
       " 453: 'dlaczego',\n",
       " 454: 'kraju',\n",
       " 455: 'rycerz',\n",
       " 456: 'byłoby',\n",
       " 457: 'wraz',\n",
       " 458: 'głowie',\n",
       " 459: 'samej',\n",
       " 460: 'czemu',\n",
       " 461: 'widząc',\n",
       " 462: 'tobie',\n",
       " 463: 'zwłaszcza',\n",
       " 464: 'miasta',\n",
       " 465: 'mojej',\n",
       " 466: 'mego',\n",
       " 467: 'wasza',\n",
       " 468: 'nasz',\n",
       " 469: 'końcu',\n",
       " 470: 'stronę',\n",
       " 471: 'wody',\n",
       " 472: 'drugiej',\n",
       " 473: 'proszę',\n",
       " 474: 'chcesz',\n",
       " 475: 'pewnie',\n",
       " 476: 'pracy',\n",
       " 477: 'ks',\n",
       " 478: 'moją',\n",
       " 479: 'wyszedł',\n",
       " 480: 'mamy',\n",
       " 481: 'widocznie',\n",
       " 482: 'daleko',\n",
       " 483: 'koło',\n",
       " 484: 'usta',\n",
       " 485: 'przynajmniej',\n",
       " 486: 'szczęście',\n",
       " 487: 'jakoby',\n",
       " 488: 'naszych',\n",
       " 489: 'mówiąc',\n",
       " 490: 'pół',\n",
       " 491: 'jakim',\n",
       " 492: 'łatwo',\n",
       " 493: 'moim',\n",
       " 494: 'wolno',\n",
       " 495: 'fabrycy',\n",
       " 496: 'panowie',\n",
       " 497: 'one',\n",
       " 498: 'wziął',\n",
       " 499: 'podniósł',\n",
       " 500: 'ręku',\n",
       " 501: 'wprost',\n",
       " 502: 'idzie',\n",
       " 503: 'księżna',\n",
       " 504: 'mówi',\n",
       " 505: 'potrzeba',\n",
       " 506: 'moich',\n",
       " 507: 'młody',\n",
       " 508: 'kobiety',\n",
       " 509: 'jakąś',\n",
       " 510: 'wszystkiego',\n",
       " 511: 'zamku',\n",
       " 512: 'oczu',\n",
       " 513: 'znalazł',\n",
       " 514: 'wiedzieć',\n",
       " 515: 'sprawy',\n",
       " 516: 'kapitan',\n",
       " 517: 'wkrótce',\n",
       " 518: 'pomyślał',\n",
       " 519: 'odezwał',\n",
       " 520: 'cicho',\n",
       " 521: 'sami',\n",
       " 522: 'rzucił',\n",
       " 523: 'trudno',\n",
       " 524: 'widzieć',\n",
       " 525: 'swych',\n",
       " 526: 'rênal',\n",
       " 527: 'szedł',\n",
       " 528: 'taka',\n",
       " 529: 'powodu',\n",
       " 530: 'uczynić',\n",
       " 531: 'swe',\n",
       " 532: 'całym',\n",
       " 533: 'pewien',\n",
       " 534: 'waćpan',\n",
       " 535: 'byłby',\n",
       " 536: 'tysięcy',\n",
       " 537: 'parę',\n",
       " 538: 'istotnie',\n",
       " 539: 'gdybym',\n",
       " 540: 'zaledwie',\n",
       " 541: 'wojska',\n",
       " 542: 'znaczy',\n",
       " 543: 'naszej',\n",
       " 544: 'słów',\n",
       " 545: 'poszedł',\n",
       " 546: 'chciała',\n",
       " 547: 'zwrócił',\n",
       " 548: 'będziesz',\n",
       " 549: 'istocie',\n",
       " 550: 'ust',\n",
       " 551: 'skrzetuski',\n",
       " 552: 'wielkiego',\n",
       " 553: 'stąd',\n",
       " 554: 'da',\n",
       " 555: 'został',\n",
       " 556: 'spokojnie',\n",
       " 557: 'wprawdzie',\n",
       " 558: 'nazajutrz',\n",
       " 559: 'mówię',\n",
       " 560: 'zarazem',\n",
       " 561: 'rzecze',\n",
       " 562: 'naprawdę',\n",
       " 563: 'nikogo',\n",
       " 564: 'stała',\n",
       " 565: 'samą',\n",
       " 566: 'naszego',\n",
       " 567: 'nowe',\n",
       " 568: 'dzieje',\n",
       " 569: 'krzyknął',\n",
       " 570: 'przyjdzie',\n",
       " 571: 'nań',\n",
       " 572: 'ducha',\n",
       " 573: 'miasto',\n",
       " 574: 'zaręba',\n",
       " 575: 'stanął',\n",
       " 576: 'rafał',\n",
       " 577: 'kogo',\n",
       " 578: 'lud',\n",
       " 579: 'tutaj',\n",
       " 580: 'część',\n",
       " 581: 'wszedł',\n",
       " 582: 'oczywiście',\n",
       " 583: 'koni',\n",
       " 584: 'kiedyś',\n",
       " 585: 'mówiła',\n",
       " 586: 'szybko',\n",
       " 587: 'winicjusz',\n",
       " 588: 'chodzi',\n",
       " 589: 'byłem',\n",
       " 590: 'chcąc',\n",
       " 591: 'żyć',\n",
       " 592: 'przyszło',\n",
       " 593: 'inny',\n",
       " 594: 'juliana',\n",
       " 595: 'ha',\n",
       " 596: 'dzisiaj',\n",
       " 597: 'męża',\n",
       " 598: 'górę',\n",
       " 599: 'spojrzał',\n",
       " 600: 'uczynił',\n",
       " 601: 'razu',\n",
       " 602: 'innym',\n",
       " 603: 'widziałem',\n",
       " 604: 'żadnej',\n",
       " 605: 'powiem',\n",
       " 606: 'pewnego',\n",
       " 607: 'gdzieś',\n",
       " 608: 'przerwał',\n",
       " 609: 'siła',\n",
       " 610: 'prócz',\n",
       " 611: 'czyż',\n",
       " 612: 'dobre',\n",
       " 613: 'ciało',\n",
       " 614: 'poczęła',\n",
       " 615: 'niczego',\n",
       " 616: 'roku',\n",
       " 617: 'bogu',\n",
       " 618: 'byle',\n",
       " 619: 'możesz',\n",
       " 620: 'godziny',\n",
       " 621: 'drugiego',\n",
       " 622: 'tobą',\n",
       " 623: 'mały',\n",
       " 624: 'miałem',\n",
       " 625: 'rodzaju',\n",
       " 626: 'duszę',\n",
       " 627: 'skąd',\n",
       " 628: 'panna',\n",
       " 629: 'wczoraj',\n",
       " 630: 'cztery',\n",
       " 631: 'takiej',\n",
       " 632: 'twoje',\n",
       " 633: 'równie',\n",
       " 634: 'imię',\n",
       " 635: 'żem',\n",
       " 636: 'rzeczypospolitej',\n",
       " 637: 'stefan',\n",
       " 638: 'rąk',\n",
       " 639: 'żadnego',\n",
       " 640: 'panem',\n",
       " 641: 'wciąż',\n",
       " 642: 'miało',\n",
       " 643: 'matka',\n",
       " 644: 'nieraz',\n",
       " 645: 'stały',\n",
       " 646: 'dusza',\n",
       " 647: 'praw',\n",
       " 648: 'słońce',\n",
       " 649: 'całego',\n",
       " 650: 'pytał',\n",
       " 651: 'wszędzie',\n",
       " 652: 'la',\n",
       " 653: 'widzi',\n",
       " 654: 'dobry',\n",
       " 655: 'krzysztof',\n",
       " 656: 'rad',\n",
       " 657: 'żołnierze',\n",
       " 658: 'znać',\n",
       " 659: 'jakaś',\n",
       " 660: 'mię',\n",
       " 661: 'części',\n",
       " 662: 'tuż',\n",
       " 663: 'wnet',\n",
       " 664: 'zamiast',\n",
       " 665: 'siedział',\n",
       " 666: 'podczas',\n",
       " 667: 'ujrzał',\n",
       " 668: 'źle',\n",
       " 669: 'znak',\n",
       " 670: 'sercu',\n",
       " 671: 'zwykle',\n",
       " 672: 'swym',\n",
       " 673: 'syna',\n",
       " 674: 'koń',\n",
       " 675: 'same',\n",
       " 676: 'ach',\n",
       " 677: 'póki',\n",
       " 678: 'mocno',\n",
       " 679: 'sił',\n",
       " 680: 'spod',\n",
       " 681: 'poczęli',\n",
       " 682: 'żal',\n",
       " 683: 'przyszedł',\n",
       " 684: 'prędko',\n",
       " 685: 'wielkim',\n",
       " 686: 'waszmość',\n",
       " 687: 'trzech',\n",
       " 688: 'drzewa',\n",
       " 689: 'atanazy',\n",
       " 690: 'myślę',\n",
       " 691: 'siłą',\n",
       " 692: 'jakiegoś',\n",
       " 693: 'jéj',\n",
       " 694: 'szczęścia',\n",
       " 695: 'prędzej',\n",
       " 696: 'powinien',\n",
       " 697: 'żywo',\n",
       " 698: 'następnie',\n",
       " 699: 'ręki',\n",
       " 700: 'całkiem',\n",
       " 701: 'żołnierz',\n",
       " 702: 'ogień',\n",
       " 703: 'niekiedy',\n",
       " 704: 'wieczorem',\n",
       " 705: 'ciała',\n",
       " 706: 'wielce',\n",
       " 707: 'dom',\n",
       " 708: 'pierwsze',\n",
       " 709: 'umiał',\n",
       " 710: 'swann',\n",
       " 711: 'twój',\n",
       " 712: 'wrażenie',\n",
       " 713: 'nasza',\n",
       " 714: 'mąż',\n",
       " 715: 'każdego',\n",
       " 716: 'niemu',\n",
       " 717: 'śmiał',\n",
       " 718: 'panią',\n",
       " 719: 'któż',\n",
       " 720: 'miały',\n",
       " 721: 'daj',\n",
       " 722: 'zrobić',\n",
       " 723: 'ono',\n",
       " 724: 'wielkiej',\n",
       " 725: 'słyszał',\n",
       " 726: 'jakimś',\n",
       " 727: 'natomiast',\n",
       " 728: 'drugim',\n",
       " 729: 'sprawę',\n",
       " 730: 'rano',\n",
       " 731: 'początku',\n",
       " 732: 'mogły',\n",
       " 733: 'sto',\n",
       " 734: 'wtem',\n",
       " 735: 'kroków',\n",
       " 736: 'jesteśmy',\n",
       " 737: 'pierwszej',\n",
       " 738: 'słychać',\n",
       " 739: 'jechać',\n",
       " 740: 'pałacu',\n",
       " 741: 'izby',\n",
       " 742: 'głośno',\n",
       " 743: 'listy',\n",
       " 744: 'około',\n",
       " 745: 'któremu',\n",
       " 746: 'łzy',\n",
       " 747: 'stronie',\n",
       " 748: 'dobra',\n",
       " 749: 'starego',\n",
       " 750: 'stać',\n",
       " 751: 'rady',\n",
       " 752: 'droga',\n",
       " 753: 'dokoła',\n",
       " 754: 'dziecko',\n",
       " 755: 'szukać',\n",
       " 756: 'natury',\n",
       " 757: 'żaden',\n",
       " 758: 'względem',\n",
       " 759: 'owe',\n",
       " 760: 'własnej',\n",
       " 761: 'wielka',\n",
       " 762: 'brat',\n",
       " 763: 'rabiego',\n",
       " 764: 'wrócił',\n",
       " 765: 'czynić',\n",
       " 766: 'warszawy',\n",
       " 767: 'musiała',\n",
       " 768: 'mieście',\n",
       " 769: 'zapewne',\n",
       " 770: 'końca',\n",
       " 771: 'szepnął',\n",
       " 772: 'dworze',\n",
       " 773: 'każdej',\n",
       " 774: 'matki',\n",
       " 775: 'żołnierzy',\n",
       " 776: 'żyje',\n",
       " 777: 'dzięki',\n",
       " 778: 'oka',\n",
       " 779: 'przeciwko',\n",
       " 780: 'mając',\n",
       " 781: 'rozkaz',\n",
       " 782: 'szlachta',\n",
       " 783: 'winnetou',\n",
       " 784: 'rozmowy',\n",
       " 785: 'najbardziej',\n",
       " 786: 'jan',\n",
       " 787: 'dawno',\n",
       " 788: 'powoli',\n",
       " 789: 'niebo',\n",
       " 790: 'poczęły',\n",
       " 791: 'wieczór',\n",
       " 792: 'znaleźć',\n",
       " 793: 'wolna',\n",
       " 794: 'obu',\n",
       " 795: 'jaka',\n",
       " 796: 'ostatni',\n",
       " 797: 'hrabia',\n",
       " 798: 'światło',\n",
       " 799: 'chcieli',\n",
       " 800: 'jakiego',\n",
       " 801: 'zatem',\n",
       " 802: 'morza',\n",
       " 803: 'znam',\n",
       " 804: 'gotów',\n",
       " 805: 'własne',\n",
       " 806: 'walki',\n",
       " 807: 'pieniądze',\n",
       " 808: 'pierwszym',\n",
       " 809: 'dwadzieścia',\n",
       " 810: 'wojny',\n",
       " 811: 'powietrze',\n",
       " 812: 'otóż',\n",
       " 813: 'mogłem',\n",
       " 814: 'niegdyś',\n",
       " 815: 'gniew',\n",
       " 816: 'zbliżył',\n",
       " 817: 'widzisz',\n",
       " 818: 'jakoż',\n",
       " 819: 'daje',\n",
       " 820: 'waść',\n",
       " 821: 'czegoś',\n",
       " 822: 'odpowiedzi',\n",
       " 823: 'mówią',\n",
       " 824: 'maćko',\n",
       " 825: 'dużo',\n",
       " 826: 'nareszcie',\n",
       " 827: 'ażeby',\n",
       " 828: 'dworu',\n",
       " 829: 'słowem',\n",
       " 830: 'dziesięć',\n",
       " 831: 'ramiona',\n",
       " 832: 'słuchał',\n",
       " 833: 'pięć',\n",
       " 834: 'myśleć',\n",
       " 835: 'prawdę',\n",
       " 836: 'ruszył',\n",
       " 837: 'syn',\n",
       " 838: 'obaj',\n",
       " 839: 'oko',\n",
       " 840: 'waszej',\n",
       " 841: 'naszym',\n",
       " 842: 'wstał',\n",
       " 843: 'hetman',\n",
       " 844: 'włosy',\n",
       " 845: 'nadziei',\n",
       " 846: 'wiatr',\n",
       " 847: 'robić',\n",
       " 848: 'patrzył',\n",
       " 849: 'powiedz',\n",
       " 850: 'ponad',\n",
       " 851: 'pomiędzy',\n",
       " 852: 'siłę',\n",
       " 853: 'drogą',\n",
       " 854: 'głowa',\n",
       " 855: 'zdawał',\n",
       " 856: 'patrząc',\n",
       " 857: 'rana',\n",
       " 858: 'zgoła',\n",
       " 859: 'nemo',\n",
       " 860: 'dawniej',\n",
       " 861: 'godzinę',\n",
       " 862: 'czem',\n",
       " 863: 'żony',\n",
       " 864: 'którymi',\n",
       " 865: 'znał',\n",
       " 866: 'petroniusz',\n",
       " 867: 'brak',\n",
       " 868: 'drugą',\n",
       " 869: 'panów',\n",
       " 870: 'pawła',\n",
       " 871: 'jednakże',\n",
       " 872: 'wielkich',\n",
       " 873: 'zbyszko',\n",
       " 874: 'nóg',\n",
       " 875: 'ludu',\n",
       " 876: 'naraz',\n",
       " 877: 'będziemy',\n",
       " 878: 'sali',\n",
       " 879: 'pierwszego',\n",
       " 880: 'fabrycego',\n",
       " 881: 'miarę',\n",
       " 882: 'samych',\n",
       " 883: 'głosy',\n",
       " 884: 'jakiejś',\n",
       " 885: 'ognia',\n",
       " 886: 'pomimo',\n",
       " 887: 'wyraz',\n",
       " 888: 'pomocą',\n",
       " 889: 'lada',\n",
       " 890: 'kobieta',\n",
       " 891: 'pełne',\n",
       " 892: 'staje',\n",
       " 893: 'okna',\n",
       " 894: 'niczym',\n",
       " 895: 'pomoc',\n",
       " 896: 'ot',\n",
       " 897: 'spokój',\n",
       " 898: 'stan',\n",
       " 899: 'doskonale',\n",
       " 900: 'głębi',\n",
       " 901: 'królowi',\n",
       " 902: 'poznać',\n",
       " 903: 'stron',\n",
       " 904: 'jakich',\n",
       " 905: 'milczeniu',\n",
       " 906: 'strach',\n",
       " 907: 'wojewoda',\n",
       " 908: 'czekać',\n",
       " 909: 'twarzą',\n",
       " 910: 'ostatnie',\n",
       " 911: 'brata',\n",
       " 912: 'dwór',\n",
       " 913: 'pytanie',\n",
       " 914: 'królewna',\n",
       " 915: 'własną',\n",
       " 916: 'człowiekiem',\n",
       " 917: 'królowa',\n",
       " 918: 'chciałem',\n",
       " 919: 'jemu',\n",
       " 920: 'toteż',\n",
       " 921: 'rycerza',\n",
       " 922: 'nowo',\n",
       " 923: 'różne',\n",
       " 924: 'wiadomo',\n",
       " 925: 'conseil',\n",
       " 926: 'nowy',\n",
       " 927: 'nieba',\n",
       " 928: 'zrobił',\n",
       " 929: 'oraz',\n",
       " 930: 'postanowił',\n",
       " 931: 'ludźmi',\n",
       " 932: 'ba',\n",
       " 933: 'celu',\n",
       " 934: 'mógłby',\n",
       " 935: 'wziąć',\n",
       " 936: 'pomocy',\n",
       " 937: 'los',\n",
       " 938: 'kraj',\n",
       " 939: 'twarze',\n",
       " 940: 'dłużej',\n",
       " 941: 'wolę',\n",
       " 942: 'jakiej',\n",
       " 943: 'morze',\n",
       " 944: 'nowego',\n",
       " 945: 'jakoś',\n",
       " 946: 'odpowiedź',\n",
       " 947: 'pokój',\n",
       " 948: 'stóp',\n",
       " 949: 'powiadał',\n",
       " 950: 'słuchać',\n",
       " 951: 'rzekłszy',\n",
       " 952: 'tymi',\n",
       " 953: 'uwagę',\n",
       " 954: 'mość',\n",
       " 955: 'radości',\n",
       " 956: 'działo',\n",
       " 957: 'naród',\n",
       " 958: 'zauważył',\n",
       " 959: 'mogąc',\n",
       " 960: 'dawał',\n",
       " 961: 'ależ',\n",
       " 962: 'szli',\n",
       " 963: 'chwilami',\n",
       " 964: 'księżnej',\n",
       " 965: 'nikomu',\n",
       " 966: 'toż',\n",
       " 967: 'sprawa',\n",
       " 968: 'długie',\n",
       " 969: 'basia',\n",
       " 970: 'myślą',\n",
       " 971: 'albowiem',\n",
       " 972: 'dnie',\n",
       " 973: 'swojego',\n",
       " 974: 'kościoła',\n",
       " 975: 'zostać',\n",
       " 976: 'piękna',\n",
       " 977: 'podobne',\n",
       " 978: 'życiem',\n",
       " 979: 'niezmiernie',\n",
       " 980: 'przykład',\n",
       " 981: 'przyszła',\n",
       " 982: 'żeś',\n",
       " 983: 'niema',\n",
       " 984: 'księdza',\n",
       " 985: 'rozum',\n",
       " 986: 'milczenie',\n",
       " 987: 'wina',\n",
       " 988: 'prawdy',\n",
       " 989: 'wołał',\n",
       " 990: 'radość',\n",
       " 991: 'ben',\n",
       " 992: 'powtórzył',\n",
       " 993: 'ciężko',\n",
       " 994: 'wszelako',\n",
       " 995: 'zdrowie',\n",
       " 996: 'owych',\n",
       " 997: 'wiadomości',\n",
       " 998: 'pewna',\n",
       " 999: 'kobiet',\n",
       " 1000: 'wszelkie',\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lookup dictionary\n",
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tensorflow Dataset instead of Generator with categorization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with model.fit: https://stackoverflow.com/questions/56604825/keras-invalidargumenterror-with-model-fit. Sequential type of input is incorrect for parallel processing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataGenerator(Sequence):\n",
    "#     def __init__(self, x_set, y_set, batch_size):\n",
    "#         self.x, self.y = x_set, y_set\n",
    "#         #self.x, self.y = tf.expand_dims(x_set, -1), tf.expand_dims(y_set, -1)\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#         #it would be possible here to additionally normalize the values -> batch_x = batch_x / float(total_words) \n",
    "#         batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#         batch_y = ku.to_categorical(batch_y, num_classes=total_words+1)\n",
    "#         return batch_x, batch_y\n",
    "\n",
    "# train_gen = DataGenerator(predictors, label, 1)  #the smallest possible batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = train_gen.__getitem__(0)\n",
    "# a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(input_sequences)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=256)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(lens2)\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    print(text.shape)\n",
    "    predictors, labels = text[:, :-1], text[:, 1:]    #offset by one + label is long!\n",
    "    print(predictors.shape, labels.shape)\n",
    "    return predictors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 41, 1)\n",
      "(None, 40, 1) (None, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocessing)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for entry in train_dataset.take(1):\n",
    "#     print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Small model with TokenAndPositionEmbedding layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transf_input (InputLayer)   [(None, 40)]              0         \n",
      "                                                                 \n",
      " transf_embed (TokenAndPosit  (None, 40, 32)           7126304   \n",
      " ionEmbedding)                                                   \n",
      "                                                                 \n",
      " transf_decod (TransformerDe  (None, 40, 32)           6464      \n",
      " coder)                                                          \n",
      "                                                                 \n",
      " transf_dense (Dense)        (None, 40, 222657)        7347681   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,480,449\n",
      "Trainable params: 14,480,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  #inicially 128\n",
    "num_heads = 4\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(maxlen, ), dtype=tf.int32, name='transf_input')\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(total_words, maxlen, embed_dim, name='transf_embed')(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5, \n",
    "                                                            name='transf_decod')(embedding_layer)\n",
    "    outputs = keras.layers.Dense(total_words, activation='softmax', name='transf_dense')(decoder)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_transf = create_model()\n",
    "model_transf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parameters: batch_size = 8, epochs = 5, embed_dim = 32, num_heads = 4 -> generate nans as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parameters: batch_size = 16, epochs = 2, embed_dim = 32, num_heads = 4 -> generate ResourceExhaustedError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parameters: batch_size = 8, epochs = 2, embed_dim = 64, num_heads = 4 -> generate ResourceExhaustedError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parameters: batch_size = 8, epochs = 2, embed_dim = 32, num_heads = 4, Adam learning_rate=0.0003 (inicially 0.001) -> generate nans as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parameters: batch_size = 8, epochs = 1, embed_dim = 32, num_heads = 4, Adam learning_rate=0.0001 (inicially 0.001) -> generate nans as weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_transf.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error with layer names: https://stackoverflow.com/questions/73187155/valueerror-unable-to-create-dataset-name-already-exists ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transf_embed/embeddings:0'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/72776335/valueerror-unable-to-create-dataset-name-already-exists-when-using-modelcheck\n",
    "\n",
    "# for i in range(len(model_transf.weights)):\n",
    "#     model_transf.weights[i]._handle_name = model_transf.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "model_transf.weights[1]._handle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_transf.export(transformer)    #AttributeError: 'Functional' object has no attribute 'export'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem discovered is nans as weights. Looks like a gradient explosion in a tiny model.\n",
    "\n",
    "https://stackoverflow.com/questions/66542007/transformer-model-output-nan-values-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'transf_embed/embeddings:0' shape=(222657, 32) dtype=float32, numpy=\n",
       "array([[        nan,         nan,         nan, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [        nan,         nan,         nan, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [        nan,         nan,         nan, ...,         nan,\n",
       "                nan,         nan],\n",
       "       ...,\n",
       "       [ 0.00542013,  0.00179588, -0.00123428, ...,  0.00217171,\n",
       "         0.00526386,  0.00295905],\n",
       "       [-0.00165877, -0.00269027, -0.00138578, ...,  0.00394636,\n",
       "        -0.00650978,  0.00835048],\n",
       "       [ 0.00433999,  0.00709753,  0.00513451, ..., -0.0047528 ,\n",
       "        -0.00021433, -0.00423851]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transf.weights[0]   #nans!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About saving problem - after changing the layer names for unique ones, and weights names for unique ones, the model can be saved without optimizer weights (crucial in case of further training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/62169315/runtimeerror-unable-to-create-link-name-already-exists-keras\n",
    "\n",
    "#https://stackoverflow.com/questions/67321942/tensorflow-2x-what-exactly-does-the-parameter-include-optimizer-affect-in-tenso\n",
    "# Saving the optimizer parameters allows you to restart training in exactly the same state as you saved the checkpoint, \n",
    "# whereas without saving the optimizer state, even the same model parameters might result in a variety of training outcomes \n",
    "# with different optimizer parameters.\n",
    "\n",
    "#model_transf.save(\"transformer.keras\", include_optimizer=False)\n",
    "loaded_model_transf = tf.keras.models.load_model(\"transformer.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After changing optimizer weights names for unique ones, the model is finally saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adam/iter:0_0'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(model_transf.optimizer.weights)):\n",
    "#     model_transf.optimizer.weights[i]._handle_name = model_transf.optimizer.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "model_transf.optimizer.weights[0]._handle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_transf.save(\"transformer_full.keras\")\n",
    "loaded_model_transf = tf.keras.models.load_model(\"transformer_full.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model with two decoders.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transf_input_b (InputLayer)  [(None, 40)]             0         \n",
      "                                                                 \n",
      " transf_embed_b (TokenAndPos  (None, 40, 8)            1781576   \n",
      " itionEmbedding)                                                 \n",
      "                                                                 \n",
      " transf_decod1_b (Transforme  (None, 40, 8)            464       \n",
      " rDecoder)                                                       \n",
      "                                                                 \n",
      " transf_decod2_b (Transforme  (None, 40, 8)            464       \n",
      " rDecoder)                                                       \n",
      "                                                                 \n",
      " transf_dropout_b (Dropout)  (None, 40, 8)             0         \n",
      "                                                                 \n",
      " transf_dense_b (Dense)      (None, 40, 222657)        2003913   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,786,417\n",
      "Trainable params: 3,786,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 8  #inicially 128\n",
    "num_heads = 2\n",
    "\n",
    "def create_model_2():\n",
    "    inputs = keras.layers.Input(shape=(maxlen, ), dtype=tf.int32, name='transf_input_b')\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(total_words, maxlen, embed_dim, name='transf_embed_b')(inputs)\n",
    "    decoder1 = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads,\n",
    "                                                            name='transf_decod1_b')(embedding_layer)\n",
    "    decoder2 = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads,\n",
    "                                                            name='transf_decod2_b')(decoder1)     \n",
    "    dropout = keras.layers.Dropout(0.5, name='transf_dropout_b')(decoder2)                                                   \n",
    "    outputs = keras.layers.Dense(total_words, activation='softmax', name='transf_dense_b')(dropout)\n",
    "    \n",
    "    model2 = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model2.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model2\n",
    "\n",
    "model_transf_2 = create_model_2()\n",
    "model_transf_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "33893/33893 [==============================] - 3491s 103ms/step - loss: nan - perplexity: nan - accuracy: 0.6745\n",
      "Epoch 2/5\n",
      " 1708/33893 [>.............................] - ETA: 55:32 - loss: nan - perplexity: nan - accuracy: 0.6681"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history2 \u001b[39m=\u001b[39m model_transf_2\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\piat-cuda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history2 = model_transf_2.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **II**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenizer. No punctuation included (restricted number of words - 100 000).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words: 222657\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Tokenizer on the Corpus\n",
    "tokenizer_restricted = Tokenizer(num_words=100000)\n",
    "tokenizer_restricted.fit_on_texts(sentences_short)\n",
    "\n",
    "# Vocabulary count of the corpus\n",
    "total_words = len(tokenizer_restricted.word_index)\n",
    "\n",
    "print(\"Total Unique Words:\", total_words)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the text into embeddings\n",
    "input_sequences = []\n",
    "for sentence in sentences_short:\n",
    "    token_list = tokenizer_restricted.texts_to_sequences([sentence])[0]\n",
    "    input_sequences.append(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(lens2)\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=maxlen+1, padding='pre'))  #maxlen +1\n",
    "\n",
    "# predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "# #label = ku.to_categorical(label1, num_classes=total_words+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(input_sequences)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=256)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(lens2)\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    print(text.shape)\n",
    "    predictors, labels = text[:, :-1], text[:, 1:]    #offset by one + label is long!\n",
    "    return predictors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 41, 1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocessing)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transf_input (InputLayer)   [(None, 40)]              0         \n",
      "                                                                 \n",
      " transf_embed (TokenAndPosit  (None, 40, 32)           7126304   \n",
      " ionEmbedding)                                                   \n",
      "                                                                 \n",
      " transf_decod (TransformerDe  (None, 40, 32)           6464      \n",
      " coder)                                                          \n",
      "                                                                 \n",
      " transf_dense (Dense)        (None, 40, 222657)        7347681   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,480,449\n",
      "Trainable params: 14,480,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  #inicially 128\n",
    "num_heads = 4\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(maxlen, ), dtype=tf.int32, name='transf_input')\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(total_words, maxlen, embed_dim, name='transf_embed')(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5, \n",
    "                                                            name='transf_decod')(embedding_layer)\n",
    "    outputs = keras.layers.Dense(total_words, activation='softmax', name='transf_dense')(decoder)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_transf = create_model()\n",
    "model_transf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33893/33893 [==============================] - 3998s 118ms/step - loss: 2.9683 - perplexity: 19.4587 - accuracy: 0.6932\n",
      "Epoch 2/3\n",
      "33893/33893 [==============================] - 3890s 115ms/step - loss: 2.6074 - perplexity: 13.5640 - accuracy: 0.7038\n",
      "Epoch 3/3\n",
      "33893/33893 [==============================] - 3868s 114ms/step - loss: 2.5367 - perplexity: 12.6376 - accuracy: 0.7069\n"
     ]
    }
   ],
   "source": [
    "history = model_transf.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'transf_embed/embeddings:0' shape=(222657, 32) dtype=float32, numpy=\n",
       " array([[-0.16373327, -0.07548821,  0.01794494, ..., -0.10588541,\n",
       "         -0.04432936, -0.05204703],\n",
       "        [-0.01211155, -0.05791748, -0.0364533 , ..., -0.0373821 ,\n",
       "          0.22052361, -0.03295827],\n",
       "        [ 0.05500536, -0.09277041, -0.1420212 , ...,  0.02098169,\n",
       "         -0.05133377,  0.03100514],\n",
       "        ...,\n",
       "        [-0.00215005, -0.00211307, -0.00230877, ..., -0.00181497,\n",
       "         -0.00453821,  0.00380428],\n",
       "        [ 0.0042766 ,  0.00036232, -0.0020407 , ...,  0.00509356,\n",
       "         -0.0043844 ,  0.00034125],\n",
       "        [ 0.00152116, -0.00226542,  0.00349827, ...,  0.00377238,\n",
       "         -0.00101423,  0.00429469]], dtype=float32)>,\n",
       " <tf.Variable 'transf_embed/embeddings:0' shape=(40, 32) dtype=float32, numpy=\n",
       " array([[-0.03179058,  0.00981777, -0.00567404, ..., -0.23976088,\n",
       "          0.03541889,  0.06951675],\n",
       "        [-0.01757253, -0.08308528, -0.00318504, ..., -0.1154981 ,\n",
       "          0.00413809, -0.00336262],\n",
       "        [ 0.01199569, -0.12539722,  0.03114812, ..., -0.08338193,\n",
       "         -0.01266156, -0.02241338],\n",
       "        ...,\n",
       "        [-0.02954685, -0.00195812, -0.03787256, ..., -0.01529739,\n",
       "          0.01802404, -0.1202457 ],\n",
       "        [-0.01311804,  0.01094438, -0.05477995, ..., -0.02643497,\n",
       "          0.03863422, -0.11975273],\n",
       "        [ 0.01327195,  0.01581597,  0.03298732, ...,  0.05110719,\n",
       "          0.16930325,  0.05067646]], dtype=float32)>,\n",
       " <tf.Variable 'transf_decod/cached_multi_head_attention_1/query/kernel:0' shape=(32, 4, 8) dtype=float32, numpy=\n",
       " array([[[-0.48218837,  0.39494047, -0.3919682 , ..., -0.48787594,\n",
       "          -0.56158966, -0.10210847],\n",
       "         [-0.6555018 ,  0.11250133, -0.6471812 , ...,  0.7445171 ,\n",
       "           0.10781173, -0.67671   ],\n",
       "         [ 0.1080104 ,  0.14598201, -0.02357555, ...,  0.28488576,\n",
       "           0.33677906, -0.2968901 ],\n",
       "         [-0.17556053,  0.20580448,  0.28189313, ...,  0.16994908,\n",
       "          -0.24468826, -0.18575181]],\n",
       " \n",
       "        [[-0.2333502 ,  0.18270917, -0.4111491 , ..., -0.23458906,\n",
       "          -0.1848819 , -0.03420627],\n",
       "         [-0.5646788 ,  0.08830249, -0.35222852, ...,  0.40942165,\n",
       "           0.3431138 , -0.39016956],\n",
       "         [ 0.31486365,  0.54848754,  0.04970248, ...,  0.19622257,\n",
       "           0.5084388 , -0.23864739],\n",
       "         [ 0.1235911 ,  0.3539087 , -0.05977948, ..., -0.24655579,\n",
       "           0.08438813,  0.01477776]],\n",
       " \n",
       "        [[-0.10020252,  0.1445621 , -0.18236879, ..., -0.22942245,\n",
       "          -0.15502481, -0.01993876],\n",
       "         [-0.13505264, -0.242195  ,  0.03850639, ...,  0.24694306,\n",
       "          -0.05419002, -0.07378782],\n",
       "         [ 0.8948599 ,  0.6111127 , -0.5041538 , ...,  0.46336386,\n",
       "           0.73930323, -0.77408   ],\n",
       "         [ 0.40829593,  0.23550223, -0.23282273, ..., -0.15031926,\n",
       "           0.21877703,  0.30787843]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.01246748, -0.21847974,  0.19162275, ...,  0.01879962,\n",
       "          -0.17625982, -0.15975927],\n",
       "         [ 0.05017457,  0.11048319,  0.13061258, ..., -0.02450881,\n",
       "          -0.02255799, -0.01386241],\n",
       "         [-0.63508964, -0.556963  , -0.1677036 , ..., -0.19299036,\n",
       "          -0.2667302 , -0.24217123],\n",
       "         [-0.4530832 , -0.4599839 ,  0.44736546, ...,  0.4848983 ,\n",
       "          -0.3824134 , -0.38925168]],\n",
       " \n",
       "        [[ 0.04376665, -0.24033973,  0.04423853, ..., -0.1665818 ,\n",
       "          -0.34107167, -0.16939211],\n",
       "         [-0.19637147,  0.2926336 , -0.1603358 , ..., -0.02040624,\n",
       "           0.11190475, -0.09017775],\n",
       "         [-0.87256813, -1.0216035 ,  0.633145  , ..., -0.49144858,\n",
       "          -0.8437463 ,  0.5965668 ],\n",
       "         [-0.5356018 , -0.71955115,  0.54498804, ...,  0.5071176 ,\n",
       "          -0.57103   , -0.4851225 ]],\n",
       " \n",
       "        [[-0.26866823,  0.29591295, -0.15551935, ..., -0.07626449,\n",
       "           0.06389461,  0.24867453],\n",
       "         [-0.20810132, -0.1659959 , -0.29094237, ...,  0.24999733,\n",
       "           0.1401084 , -0.18284956],\n",
       "         [ 0.615276  ,  0.487566  , -0.3335885 , ...,  0.44806534,\n",
       "           0.5146277 , -0.36455002],\n",
       "         [ 0.32649696,  0.65546215, -0.28553617, ..., -0.2376931 ,\n",
       "           0.37054265,  0.41473195]]], dtype=float32)>,\n",
       " <tf.Variable 'transf_decod/cached_multi_head_attention_1/query/bias:0' shape=(4, 8) dtype=float32, numpy=\n",
       " array([[-0.13859995,  0.10864215, -0.09254725, -0.10664082, -0.20057078,\n",
       "         -0.24923564, -0.3351304 ,  0.02275244],\n",
       "        [-0.05603039,  0.16183922, -0.08783565, -0.1297524 , -0.0755128 ,\n",
       "         -0.01276746, -0.01010816,  0.07311926],\n",
       "        [-0.5759704 , -0.63026804,  0.15969187, -0.63853836,  0.6033927 ,\n",
       "         -0.35043752, -0.6971819 ,  0.49516416],\n",
       "        [-0.6750944 , -0.64902574,  0.56822455,  0.6207331 , -0.5254021 ,\n",
       "          0.6538272 , -0.49540943, -0.53977567]], dtype=float32)>,\n",
       " <tf.Variable 'transf_decod/cached_multi_head_attention_1/key/kernel:0' shape=(32, 4, 8) dtype=float32, numpy=\n",
       " array([[[-9.52819586e-01,  8.05640161e-01, -6.96764171e-01, ...,\n",
       "          -8.84799063e-01, -4.89227593e-01,  3.62689078e-01],\n",
       "         [-6.96408629e-01, -2.50874180e-02, -6.69288218e-01, ...,\n",
       "           4.09921438e-01,  1.84712857e-01, -5.10489643e-01],\n",
       "         [-4.60579127e-01, -2.35207871e-01,  5.47058225e-01, ...,\n",
       "          -3.59710157e-01, -4.46567088e-01,  7.15741932e-01],\n",
       "         [-4.37318414e-01, -2.98504084e-01,  5.34990311e-01, ...,\n",
       "           5.99234462e-01, -3.95634323e-01, -5.86811662e-01]],\n",
       " \n",
       "        [[-7.87756443e-01,  7.15579212e-01, -7.76294053e-01, ...,\n",
       "          -5.86828828e-01, -3.61416310e-01,  3.36241335e-01],\n",
       "         [-4.68538314e-01,  5.76908812e-02, -5.84417045e-01, ...,\n",
       "           5.14747381e-01,  1.02285504e-01, -5.49502730e-01],\n",
       "         [-1.42533302e-01, -1.09268509e-01,  3.34284842e-01, ...,\n",
       "          -1.34665146e-01, -2.65736371e-01,  4.03699934e-01],\n",
       "         [-4.89343852e-01, -3.72721314e-01,  2.19025686e-01, ...,\n",
       "           5.36238611e-01, -2.51064688e-01, -4.11439717e-01]],\n",
       " \n",
       "        [[-2.10714325e-01,  2.50833571e-01, -3.42214078e-01, ...,\n",
       "          -2.05341399e-01, -1.16341852e-01, -6.70392364e-02],\n",
       "         [ 7.85763264e-02, -3.06868345e-01,  7.73075968e-02, ...,\n",
       "          -1.70750748e-02, -4.72128958e-01,  4.35949713e-02],\n",
       "         [ 2.03757778e-01,  9.49490294e-02,  2.48192772e-02, ...,\n",
       "           2.16450572e-01,  1.82128936e-01, -8.05357024e-02],\n",
       "         [-5.61953464e-04,  1.43193379e-01, -3.53917032e-02, ...,\n",
       "           1.32471606e-01,  1.47367686e-01,  2.84920372e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-5.91088712e-01,  5.77957809e-01, -4.21113163e-01, ...,\n",
       "          -8.00850034e-01, -4.37763691e-01,  1.50626212e-01],\n",
       "         [ 3.57039161e-02, -1.40842199e-01, -2.10518256e-01, ...,\n",
       "           1.96696654e-01, -3.33628654e-01,  1.68102141e-02],\n",
       "         [-4.87218916e-01, -4.33988273e-01,  3.48931104e-01, ...,\n",
       "          -3.86798918e-01, -2.30797097e-01,  3.30467671e-01],\n",
       "         [-2.84409553e-01, -7.19669700e-01,  5.49906075e-01, ...,\n",
       "           5.89015305e-01, -5.99748969e-01, -4.11365300e-01]],\n",
       " \n",
       "        [[-3.62376720e-01,  5.46009958e-01, -4.11998034e-01, ...,\n",
       "          -2.61922389e-01, -9.67348833e-03,  2.90683270e-01],\n",
       "         [-7.06019532e-03,  1.26510218e-03, -1.75373286e-01, ...,\n",
       "           9.57099795e-02,  1.80704892e-01,  2.94291601e-02],\n",
       "         [-1.61755481e-03,  2.33482257e-01, -1.14473127e-01, ...,\n",
       "           5.19122870e-04,  4.13482964e-01, -3.26131582e-01],\n",
       "         [ 3.31323028e-01, -9.16166306e-02, -1.27949595e-01, ...,\n",
       "          -2.26401940e-01,  1.68679357e-01,  8.69740471e-02]],\n",
       " \n",
       "        [[-1.71249047e-01,  3.37870687e-01, -3.41896266e-01, ...,\n",
       "          -1.76478282e-01,  1.26433611e-01,  7.15265870e-02],\n",
       "         [-4.31445897e-01,  5.32603078e-02, -4.69418764e-01, ...,\n",
       "           3.37086648e-01,  1.71691645e-03, -5.26608586e-01],\n",
       "         [ 2.87010878e-01,  2.21515819e-01,  3.98066372e-01, ...,\n",
       "           1.85006469e-01,  5.40746823e-02,  2.62739867e-01],\n",
       "         [-2.58430112e-02, -1.25926971e-01,  2.84132995e-02, ...,\n",
       "          -1.43957045e-02, -2.89725196e-02,  5.40357754e-02]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'transf_decod/cached_multi_head_attention_1/key/bias:0' shape=(4, 8) dtype=float32, numpy=\n",
       " array([[-0.00334891,  0.00161619, -0.00170277, -0.00064309, -0.0005969 ,\n",
       "         -0.00439494, -0.0044106 , -0.00368828],\n",
       "        [-0.00340035, -0.00403303, -0.00483455, -0.00363166, -0.00458466,\n",
       "          0.0045649 , -0.00280362, -0.00296956],\n",
       "        [-0.00215031, -0.0034845 ,  0.00442061, -0.00313099,  0.00345152,\n",
       "          0.00630151, -0.00454228,  0.00115341],\n",
       "        [ 0.00864897,  0.01022981, -0.00860445, -0.01128081,  0.01114258,\n",
       "         -0.00867003,  0.00747516,  0.00835686]], dtype=float32)>,\n",
       " <tf.Variable 'transf_decod/cached_multi_head_attention_1/value/kernel:0' shape=(32, 4, 8) dtype=float32, numpy=\n",
       " array([[[-0.04950994, -0.00528307, -0.12981302, ...,  0.04959881,\n",
       "           0.05133529,  0.10254936],\n",
       "         [-0.00205026,  0.09840617,  0.05423679, ..., -0.07920463,\n",
       "           0.00033974, -0.07816605],\n",
       "         [-0.09584751,  0.18757817, -0.04877672, ..., -0.18721794,\n",
       "           0.08445933,  0.06664128],\n",
       "         [-0.06316213,  0.08194043, -0.00627974, ...,  0.14282276,\n",
       "           0.08192667,  0.05738856]],\n",
       " \n",
       "        [[-0.04061715, -0.02003382, -0.00663898, ...,  0.04599242,\n",
       "          -0.0147765 , -0.06172225],\n",
       "         [-0.02226377, -0.08632766, -0.01505483, ..., -0.02605846,\n",
       "           0.02055035, -0.05052135],\n",
       "         [-0.03618673,  0.05099231,  0.07822953, ...,  0.07121328,\n",
       "          -0.04197732, -0.01686662],\n",
       "         [-0.03962528, -0.0704609 ,  0.07555152, ...,  0.08200331,\n",
       "          -0.06764465, -0.00769048]],\n",
       " \n",
       "        [[ 0.05047342,  0.07425723, -0.02559152, ..., -0.02702693,\n",
       "           0.065262  , -0.05510887],\n",
       "         [-0.08502069, -0.02279229,  0.03548602, ...,  0.00023772,\n",
       "          -0.05139488, -0.00779303],\n",
       "         [ 0.0150984 , -0.0468204 ,  0.08072346, ...,  0.00591267,\n",
       "          -0.07834282,  0.05813947],\n",
       "         [ 0.06409443,  0.00606204,  0.0442173 , ...,  0.04195302,\n",
       "           0.07210584, -0.07824248]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.05142149,  0.06586805, -0.03006295, ..., -0.08855473,\n",
       "           0.06072182, -0.09469411],\n",
       "         [ 0.09832591, -0.04563606, -0.10355219, ...,  0.02493212,\n",
       "          -0.07899052,  0.04461368],\n",
       "         [ 0.01534583, -0.0117172 ,  0.0160153 , ...,  0.01038599,\n",
       "           0.05365105, -0.05973951],\n",
       "         [ 0.16865918, -0.01814592,  0.01576166, ..., -0.07472309,\n",
       "           0.0416741 ,  0.03553923]],\n",
       " \n",
       "        [[-0.02589287, -0.00816518,  0.0077104 , ...,  0.00314   ,\n",
       "           0.041348  ,  0.00473514],\n",
       "         [-0.00307416,  0.0084586 ,  0.02847558, ..., -0.03451111,\n",
       "           0.02823369,  0.02471152],\n",
       "         [-0.02739038, -0.09048259,  0.1114186 , ..., -0.1284556 ,\n",
       "          -0.01234265, -0.00943169],\n",
       "         [ 0.00208016,  0.01609943, -0.03544839, ..., -0.03270994,\n",
       "          -0.00587409, -0.0758782 ]],\n",
       " \n",
       "        [[ 0.02349703,  0.10015543,  0.14220291, ..., -0.04646333,\n",
       "           0.04787495, -0.02335752],\n",
       "         [-0.05823056,  0.07283075,  0.06267798, ...,  0.05147645,\n",
       "           0.06877493,  0.06975792],\n",
       "         [ 0.05521641,  0.06557913,  0.07333785, ..., -0.03642014,\n",
       "           0.0184103 ,  0.06832869],\n",
       "         [ 0.07617788,  0.02365876, -0.08023895, ...,  0.05159033,\n",
       "           0.06026594, -0.03761282]]], dtype=float32)>,\n",
       " <tf.Variable 'transf_decod/cached_multi_head_attention_1/value/bias:0' shape=(4, 8) dtype=float32, numpy=\n",
       " array([[-3.3094052e-03,  1.3203786e-02, -5.3501790e-05,  2.1652332e-02,\n",
       "         -3.9897440e-04, -6.3264742e-03, -1.0067760e-03,  1.4497391e-02],\n",
       "        [-2.4561689e-04,  5.1948787e-03,  2.3963571e-02,  1.5411208e-03,\n",
       "          5.4354657e-04,  1.3346096e-03,  1.2690702e-02, -9.0923011e-03],\n",
       "        [ 4.6439259e-03,  7.5143622e-03,  1.2415959e-02, -6.1272597e-03,\n",
       "          1.2837850e-02,  6.5702450e-04,  3.4830149e-03,  1.3505567e-02],\n",
       "        [ 2.4731641e-03, -1.2744131e-04, -9.8753134e-03,  3.7455519e-03,\n",
       "         -7.7468893e-03,  2.2304719e-02,  2.1455737e-02, -1.7574538e-02]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'transf_decod/cached_multi_head_attention_1/attention_output/kernel:0' shape=(4, 8, 32) dtype=float32, numpy=\n",
       " array([[[-9.90709811e-02,  2.62972731e-02,  8.56979564e-02, ...,\n",
       "          -8.74399692e-02, -1.63999438e-01,  8.67136866e-02],\n",
       "         [-1.89585180e-03,  9.14472938e-02,  7.96222538e-02, ...,\n",
       "           4.75131571e-02,  1.05705976e-01,  1.42390374e-02],\n",
       "         [-3.32350358e-02,  9.09423903e-02, -1.58377495e-02, ...,\n",
       "           2.33101267e-02,  2.80363318e-02,  6.28162622e-02],\n",
       "         ...,\n",
       "         [ 4.09559086e-02,  1.11297309e-01,  4.40234924e-03, ...,\n",
       "           3.21093537e-02, -7.46496196e-04,  5.57379983e-02],\n",
       "         [-3.44911776e-02,  7.51466444e-03,  3.41591355e-03, ...,\n",
       "           7.52460584e-02,  5.94842471e-02, -4.11170423e-02],\n",
       "         [-4.32790853e-02,  3.79059054e-02, -5.10642081e-02, ...,\n",
       "           4.51252423e-02,  6.65341094e-02,  4.02353778e-02]],\n",
       " \n",
       "        [[-1.62842628e-02, -2.92508584e-02, -1.01272218e-01, ...,\n",
       "           5.23068868e-02, -3.49696167e-02, -2.61540152e-02],\n",
       "         [ 3.19864005e-02, -3.59029733e-02,  6.34138659e-03, ...,\n",
       "           1.63655111e-03,  6.11011200e-02,  9.30240899e-02],\n",
       "         [ 5.83861843e-02, -5.63750677e-02, -2.38857213e-02, ...,\n",
       "          -7.23282397e-02,  8.09925608e-03, -5.76146841e-02],\n",
       "         ...,\n",
       "         [-6.97250888e-02, -9.16672423e-02, -6.84010610e-02, ...,\n",
       "          -3.71264815e-02, -4.57480997e-02, -1.06875740e-01],\n",
       "         [ 1.55209489e-02,  1.09460458e-01,  2.45742016e-02, ...,\n",
       "           7.84959868e-02,  1.99560821e-02, -3.87279242e-02],\n",
       "         [ 2.98306881e-03,  2.63399687e-02,  2.12917458e-02, ...,\n",
       "          -1.48731589e-04,  3.43958512e-02,  1.31867185e-01]],\n",
       " \n",
       "        [[-8.43323488e-03, -2.76969168e-02,  3.76854129e-02, ...,\n",
       "          -1.17807295e-02, -9.19167995e-02, -1.35612469e-02],\n",
       "         [ 9.57027450e-02, -4.31349576e-02, -1.92296192e-01, ...,\n",
       "           8.97967443e-02,  4.06062044e-02, -7.28428771e-04],\n",
       "         [ 1.63621120e-02, -7.16133192e-02,  6.37550205e-02, ...,\n",
       "          -6.10668883e-02,  2.00993180e-01,  8.07039905e-04],\n",
       "         ...,\n",
       "         [-1.87359959e-01,  1.61101431e-01, -1.04750887e-01, ...,\n",
       "          -1.43336486e-02, -1.54419288e-01,  2.16015399e-01],\n",
       "         [ 6.17529303e-02,  4.98516373e-02, -8.63779038e-02, ...,\n",
       "           2.60106046e-02, -1.16517954e-01, -9.22842138e-03],\n",
       "         [-3.54858837e-03,  1.08873464e-01,  3.09756445e-03, ...,\n",
       "           5.90087622e-02,  5.33398353e-02, -1.29640093e-02]],\n",
       " \n",
       "        [[ 3.88500504e-02,  1.56876758e-01, -1.41664632e-02, ...,\n",
       "           1.40139297e-01,  1.33914158e-01,  1.37092754e-01],\n",
       "         [ 5.19384779e-02,  4.76790778e-02, -4.42837961e-02, ...,\n",
       "          -1.34245977e-01,  2.09767539e-02, -1.38169870e-01],\n",
       "         [ 1.21264055e-01,  5.37812114e-02, -3.43284905e-02, ...,\n",
       "          -7.45237544e-02,  2.15516891e-03,  3.78579982e-02],\n",
       "         ...,\n",
       "         [ 3.10394410e-02, -7.35810772e-02, -1.22040845e-01, ...,\n",
       "          -1.28023237e-01, -5.29459491e-02, -1.31781355e-01],\n",
       "         [ 1.30300537e-01,  7.92914350e-03,  4.76838425e-02, ...,\n",
       "           1.35540232e-01,  1.80333987e-01,  3.02361511e-02],\n",
       "         [-3.89435552e-02,  5.19584864e-02, -8.70805159e-02, ...,\n",
       "           1.60838775e-02, -1.11370258e-01,  2.11552158e-02]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'transf_decod/cached_multi_head_attention_1/attention_output/bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-1.4063334e-03,  7.3923875e-04, -2.4046733e-04,  2.8682919e-03,\n",
       "         4.8234123e-03, -1.2257680e-03, -5.0096423e-04, -3.4896757e-03,\n",
       "        -2.6666396e-03,  8.2954874e-05, -1.5973778e-04,  2.6344422e-03,\n",
       "        -8.1163045e-04,  2.4824061e-03, -9.6624589e-04, -1.5558904e-03,\n",
       "         7.9047782e-03, -3.7283499e-03,  9.9294353e-04, -7.0350082e-04,\n",
       "         7.3752645e-04,  8.9236308e-04,  2.2940908e-03,  1.8691847e-03,\n",
       "         7.4239698e-04,  2.6476393e-03, -3.6999723e-03,  1.7497203e-03,\n",
       "        -1.9323285e-03,  2.7419222e-03, -4.8683458e-03, -1.8798227e-03],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'gamma:0' shape=(32,) dtype=float32, numpy=\n",
       " array([1.269951  , 0.8458649 , 0.8241892 , 1.1360487 , 0.94505244,\n",
       "        0.8599427 , 0.83017814, 0.826286  , 1.0517001 , 0.86891097,\n",
       "        0.4679236 , 0.927579  , 0.8323144 , 1.0201195 , 1.0959413 ,\n",
       "        1.2680216 , 0.52924967, 1.0024662 , 1.0888388 , 0.8003574 ,\n",
       "        0.84920716, 0.64261776, 0.77436954, 0.93272394, 1.1827598 ,\n",
       "        1.1248558 , 0.7092387 , 0.88746125, 1.173013  , 0.9203773 ,\n",
       "        0.7346701 , 0.68208295], dtype=float32)>,\n",
       " <tf.Variable 'beta:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.17130823, -0.00633055,  0.17858022,  0.14406562,  0.29190698,\n",
       "         0.43177193, -0.05572407, -0.12009883, -0.18816522,  0.07767029,\n",
       "         0.00098261, -0.09503884, -0.4653303 , -0.05796197,  0.11661164,\n",
       "         0.01228203,  0.4228726 , -0.2606627 ,  0.00840349, -0.05813849,\n",
       "        -0.3808785 ,  0.4705876 ,  0.13099928, -0.10491219, -0.02935965,\n",
       "         0.12761293, -0.23147988, -0.38183236, -0.2192321 , -0.0993089 ,\n",
       "        -0.7317249 , -0.03903872], dtype=float32)>,\n",
       " <tf.Variable 'kernel:0' shape=(32, 32) dtype=float32, numpy=\n",
       " array([[ 0.009022  , -0.08030043, -0.0954271 , ..., -0.02891395,\n",
       "          0.09804909,  0.09308096],\n",
       "        [-0.00562984,  0.01297523,  0.07919408, ..., -0.02807583,\n",
       "          0.25083408,  0.06183108],\n",
       "        [ 0.11898507,  0.011593  ,  0.06547569, ..., -0.10085411,\n",
       "         -0.05449123, -0.0815148 ],\n",
       "        ...,\n",
       "        [-0.11673749, -0.02209071, -0.14109851, ..., -0.06190635,\n",
       "          0.18512334, -0.17486663],\n",
       "        [-0.30402797, -0.23427735,  0.01153329, ..., -0.04201426,\n",
       "         -0.11451054, -0.02563394],\n",
       "        [ 0.22134486,  0.28665063, -0.02198265, ...,  0.01379147,\n",
       "          0.08133397,  0.16494693]], dtype=float32)>,\n",
       " <tf.Variable 'bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.11343966, -0.27402624, -0.23776697, -0.10461992, -0.17953992,\n",
       "        -0.22411701, -0.07277559, -0.09083202, -0.04583482, -0.1764006 ,\n",
       "        -0.01534774, -0.2043673 , -0.04942932, -0.25514212, -0.0919198 ,\n",
       "         0.01077813, -0.09958436, -0.02287036,  0.09278288, -0.12459248,\n",
       "        -0.13359146,  0.02660317, -0.04371415, -0.08326293, -0.01754393,\n",
       "        -0.09851956, -0.14802355, -0.13023771, -0.10554474, -0.08105727,\n",
       "        -0.08554903, -0.00522588], dtype=float32)>,\n",
       " <tf.Variable 'kernel:0' shape=(32, 32) dtype=float32, numpy=\n",
       " array([[-0.06343959, -0.13212998,  0.0642807 , ..., -0.0688583 ,\n",
       "          0.11852258,  0.16855882],\n",
       "        [-0.24682672, -0.01993431,  0.0282769 , ...,  0.06248603,\n",
       "         -0.07130788, -0.07064691],\n",
       "        [-0.13721833, -0.06579161,  0.0419359 , ..., -0.12766583,\n",
       "         -0.13449636,  0.14127518],\n",
       "        ...,\n",
       "        [ 0.12172348, -0.01060851, -0.02609299, ..., -0.06010807,\n",
       "          0.07714427,  0.13561592],\n",
       "        [-0.14874843, -0.13971151, -0.11141078, ..., -0.10404571,\n",
       "         -0.06268973,  0.0557504 ],\n",
       "        [-0.188398  , -0.15121958, -0.01375891, ..., -0.23612633,\n",
       "         -0.1642879 , -0.0148703 ]], dtype=float32)>,\n",
       " <tf.Variable 'bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([ 0.05160217,  0.03062202,  0.009987  , -0.02020542,  0.01744628,\n",
       "         0.05261827, -0.02408482, -0.04169663, -0.0288899 ,  0.01257639,\n",
       "         0.03853254, -0.06349222, -0.03241934,  0.0046357 , -0.07220873,\n",
       "        -0.03233783,  0.00620646,  0.00453991, -0.06087426, -0.05534143,\n",
       "        -0.07690578,  0.01990309,  0.02439029,  0.04060551, -0.01594665,\n",
       "        -0.03643602, -0.05595936, -0.05630638, -0.03809591, -0.00094719,\n",
       "        -0.00933952,  0.00506451], dtype=float32)>,\n",
       " <tf.Variable 'gamma:0' shape=(32,) dtype=float32, numpy=\n",
       " array([2.2837334, 1.6274815, 1.96323  , 2.5019448, 2.1381617, 2.099733 ,\n",
       "        2.2065604, 1.826941 , 2.7999506, 2.3503537, 1.1300234, 1.5093435,\n",
       "        1.8458327, 2.7131994, 2.3681996, 2.8322122, 1.8287857, 2.307092 ,\n",
       "        2.088208 , 2.0624723, 2.291777 , 2.0303016, 1.6592134, 1.9749147,\n",
       "        2.0278714, 2.6315694, 1.8613536, 2.4773955, 2.6552017, 1.8716291,\n",
       "        1.8870434, 1.2310264], dtype=float32)>,\n",
       " <tf.Variable 'beta:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.0722957 ,  0.06316019, -0.23361915, -0.48695278,  0.06060531,\n",
       "         0.03544083,  0.0097025 , -0.04050982, -0.09261639,  0.08913153,\n",
       "        -0.11912177, -0.09525204, -0.03205937, -0.27031848,  0.11731784,\n",
       "         0.2529106 ,  0.4108454 , -0.20390618,  0.10427544,  0.21227393,\n",
       "         0.15734689,  0.12853731, -0.15006974, -0.00076751, -0.01343787,\n",
       "        -0.07853356,  0.04420348,  0.09180193,  0.08177826, -0.5897994 ,\n",
       "        -0.0684261 , -0.03946145], dtype=float32)>,\n",
       " <tf.Variable 'transf_dense/kernel:0' shape=(32, 222657) dtype=float32, numpy=\n",
       " array([[-0.29519832, -0.2359535 , -0.12103193, ...,  0.54781884,\n",
       "          0.5464296 ,  0.5453971 ],\n",
       "        [-0.06276006, -0.41176516, -0.15777053, ...,  1.0832273 ,\n",
       "          1.0845165 ,  1.0852607 ],\n",
       "        [ 0.17019285, -0.49594417, -0.36429718, ...,  1.1646924 ,\n",
       "          1.162467  ,  1.1628313 ],\n",
       "        ...,\n",
       "        [-0.7186064 ,  0.30816373,  0.21648595, ..., -0.4797775 ,\n",
       "         -0.4822192 , -0.4801289 ],\n",
       "        [-0.3700017 ,  0.15599526,  0.32663947, ..., -1.067979  ,\n",
       "         -1.0678047 , -1.0671481 ],\n",
       "        [ 0.23528084, -0.7054651 , -0.42033803, ...,  1.4250054 ,\n",
       "          1.4215097 ,  1.4178627 ]], dtype=float32)>,\n",
       " <tf.Variable 'transf_dense/bias:0' shape=(222657,) dtype=float32, numpy=\n",
       " array([-0.24829629,  1.1558026 ,  0.5564091 , ..., -1.7175176 ,\n",
       "        -1.7180722 , -1.7183592 ], dtype=float32)>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transf.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_transf.weights)):\n",
    "    model_transf.weights[i]._handle_name = model_transf.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_transf.optimizer.weights)):\n",
    "    model_transf.optimizer.weights[i]._handle_name = model_transf.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_transf.save(\"transformer_restricted.keras\")\n",
    "loaded_model_transf_restricted = tf.keras.models.load_model(\"transformer_restricted.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "    #print(\"logits shape: \", logits.shape)\n",
    "    logits, indices = tf.math.top_k(logits, k=15, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent words obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "za górami za lasami gdy to  gdy gdy a a po po ale kiedy  kiedy to kiedy gdy tak\n",
      "z   czy co niego się on gdy to w z na jak to ale do się siebie już że od i nemo\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "next_words = 40 #maximum 40\n",
    "\n",
    "sample_index = 0\n",
    "\n",
    "while next_words-1 > sample_index:\n",
    "\n",
    "    #embeddings\n",
    "    token_list = tokenizer_restricted.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "    #padding\n",
    "    maxlen = max(lens2)\n",
    "    test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen+1, padding='pre'))\n",
    "\n",
    "    #test sample\n",
    "    test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "    #predictions\n",
    "    soft_pred = loaded_model_transf_restricted.predict(test_sequence, verbose=0)\n",
    "    #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "    sample_index = len(seed_text.strip().split())-1\n",
    "    #print(\"sample_index\", sample_index)\n",
    "    sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "    #print(sampled_token)\n",
    "\n",
    "    output_word = \"\"\n",
    "    #decoding tokens\n",
    "    for word, index in tokenizer_restricted.word_index.items():\n",
    "        if index == sampled_token:\n",
    "            output_word = word\n",
    "            break\n",
    "    #sampled_token = index_lookup[sampled_token]\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(seed_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **More training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_restricted = tf.keras.models.load_model(\"transformer_restricted.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "33893/33893 [==============================] - 3610s 106ms/step - loss: 2.6422 - perplexity: 14.0437 - accuracy: 0.7023\n",
      "Epoch 2/3\n",
      "33893/33893 [==============================] - 3626s 107ms/step - loss: 2.5385 - perplexity: 12.6604 - accuracy: 0.7071\n",
      "Epoch 3/3\n",
      "33893/33893 [==============================] - 3623s 107ms/step - loss: 2.4885 - perplexity: 12.0434 - accuracy: 0.7090\n"
     ]
    }
   ],
   "source": [
    "history2 = loaded_model_transf_restricted.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_restricted.weights)):\n",
    "    loaded_model_transf_restricted.weights[i]._handle_name = loaded_model_transf_restricted.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_restricted.optimizer.weights)):\n",
    "    loaded_model_transf_restricted.optimizer.weights[i]._handle_name = loaded_model_transf_restricted.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_restricted.save(\"transformer_restricted2.keras\")\n",
    "loaded_model_transf_restricted2 = tf.keras.models.load_model(\"transformer_restricted2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More words used, but lots is rejected by a dictionary (blank spaces, because the words are recognized as OOV). It can be seen because for each sentence loop, the output is not of full sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "    #print(\"logits shape: \", logits.shape)\n",
    "    logits, indices = tf.math.top_k(logits, k=15, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami i to a w na po to tak i gdy z gdy po ja ja lecz ale nimi\n",
      "siebie o potem tym za po tego prostu ci co że tak pomocą dnia że ja i ma mnie\n",
      "był nie nie to lecz w a i a na nie z na i nie i to zaś do jak mógł co gdy za\n",
      "mógł świecie będzie znaczy tego go mi to z człowieka nie tych chcę tak kiedy\n",
      "kiedy i co na to na z w to – i na to z – ale co w zaś się wszystko nim był z był\n",
      "co do z w tu to głowy tym wszystko kraju w nie gdy co to ale z po a gdy i kiedy\n",
      "nie – na lecz na ale co tej król się tej za się mógł niego nią to zaś samej to\n",
      "ale że przyczyny tak woli a lecz kiedy i – – tak tak na z tak tak a nie z tak –\n",
      "na i on nie ale mi koniec nie teraz sobą i sam ma że był że się ja już tego\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_restricted.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen = max(lens2)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_restricted2.predict(test_sequence, verbose=0)\n",
    "        #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        #print(\"sample_index\", sample_index)\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "        #print(sampled_token)\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_restricted.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        #sampled_token = index_lookup[sampled_token]\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #print(seed_text)\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "    #print(full_text)\n",
    "    #print(seed_text)\n",
    "\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Even more training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_restricted2 = tf.keras.models.load_model(\"transformer_restricted2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "33893/33893 [==============================] - 3585s 106ms/step - loss: 2.6353 - perplexity: 13.9480 - accuracy: 0.7022\n",
      "Epoch 2/4\n",
      "33893/33893 [==============================] - 3585s 106ms/step - loss: 2.5323 - perplexity: 12.5826 - accuracy: 0.7070\n",
      "Epoch 3/4\n",
      "33893/33893 [==============================] - 3585s 106ms/step - loss: 2.4864 - perplexity: 12.0178 - accuracy: 0.7091\n",
      "Epoch 4/4\n",
      "33893/33893 [==============================] - 3585s 106ms/step - loss: 2.4573 - perplexity: 11.6731 - accuracy: 0.7103\n"
     ]
    }
   ],
   "source": [
    "history3 = loaded_model_transf_restricted2.fit(train_dataset, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_restricted2.weights)):\n",
    "    loaded_model_transf_restricted2.weights[i]._handle_name = loaded_model_transf_restricted2.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_restricted2.optimizer.weights)):\n",
    "    loaded_model_transf_restricted2.optimizer.weights[i]._handle_name = loaded_model_transf_restricted2.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_restricted2.save(\"transformer_restricted3.keras\")\n",
    "loaded_model_transf_restricted3 = tf.keras.models.load_model(\"transformer_restricted3.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "    #print(\"logits shape: \", logits.shape)\n",
    "    logits, indices = tf.math.top_k(logits, k=15, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami gdy a gdy ale nie ale a co tak ale lecz gdy w tak na ale ja\n",
      "chwilę to nie to ma na długo tak ręku nich się za na nich to za każdym i mną\n",
      "słowa jeśli a kiedy kiedy w a tak z tak gdy z i ale nie ale był – miejscu i pan\n",
      "go domu tak tak początku z i rzekł tego to w jest na co nich świecie może gdy i\n",
      "– nie gdy a ale ale gdy po – – a a nie ja co chwila z już wyrzekł mu co już za\n",
      "nie było za sobą ale się nic na na za niego sobą nie to z w z co gdy co i a lecz\n",
      "na w gdy w po co koniec na mogę tego jego pan to tak życiu nich by czele się\n",
      "andrzej że i tych nie tak można to po w ale gdy i na to to z nie tak co po ja\n",
      "ale gdy kilku jak na ten to to rzekłszy wiedział to nie się to człowiek na nie\n",
      "ma by koniec nie domu\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_restricted.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen = max(lens2)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_restricted3.predict(test_sequence, verbose=0)\n",
    "        #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        #print(\"sample_index\", sample_index)\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "        #print(sampled_token)\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_restricted.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        #sampled_token = index_lookup[sampled_token]\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #print(seed_text)\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "    #print(full_text)\n",
    "    #print(seed_text)\n",
    "\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **More more training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model is already trained on 10 epochs\n",
    "loaded_model_transf_restricted3 = tf.keras.models.load_model(\"transformer_restricted3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "33893/33893 [==============================] - 3607s 106ms/step - loss: 2.6387 - perplexity: 13.9943 - accuracy: 0.7017\n",
      "Epoch 2/5\n",
      "33893/33893 [==============================] - 3646s 108ms/step - loss: 2.5340 - perplexity: 12.6035 - accuracy: 0.7065\n",
      "Epoch 3/5\n",
      "33893/33893 [==============================] - 3646s 108ms/step - loss: 2.4885 - perplexity: 12.0437 - accuracy: 0.7087\n",
      "Epoch 4/5\n",
      "33893/33893 [==============================] - 3611s 107ms/step - loss: 2.4602 - perplexity: 11.7069 - accuracy: 0.7099\n",
      "Epoch 5/5\n",
      "33893/33893 [==============================] - 3583s 106ms/step - loss: 2.4407 - perplexity: 11.4815 - accuracy: 0.7109\n"
     ]
    }
   ],
   "source": [
    "history4 = loaded_model_transf_restricted3.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_restricted3.weights)):\n",
    "    loaded_model_transf_restricted3.weights[i]._handle_name = loaded_model_transf_restricted3.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_restricted3.optimizer.weights)):\n",
    "    loaded_model_transf_restricted3.optimizer.weights[i]._handle_name = loaded_model_transf_restricted3.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_restricted3.save(\"transformer_restricted4.keras\")\n",
    "loaded_model_transf_restricted4 = tf.keras.models.load_model(\"transformer_restricted4.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "    #print(\"logits shape: \", logits.shape)\n",
    "    logits, indices = tf.math.top_k(logits, k=15, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami po ja gdy a tak z a ale gdy a na co – a i ja tak późno nią\n",
      "prostu to było z go dworze się po było to to wolna go tym już do z warszawy w\n",
      "nie po gdy po z i gdy gdy a a ale tak to – a gdy była tej tem tym tym ja się nie\n",
      "mi zwrócił się mierze sposobem nie wiem mu w był na pewny to a kiedy w kiedy tak\n",
      "po – – gdy i co gdy tak i nie tak końcu świecie na mnie zaś czym co w w to długo\n",
      "o tak był niej było niej na tylko niego gdyby na nie nie a i ja w na w co po i\n",
      "po – ale nie w po był mogąc może też tem ty ja co może tym ich się się to nie\n",
      "jak być mi nie w – i ja nie kiedy na w nie tak tak i to nie po po miał z mógł\n",
      "odparł mam zaś istocie nie się wiem to nimi julian jak będzie to na nie za\n",
      "wiedział\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_restricted.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen = max(lens2)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_restricted4.predict(test_sequence, verbose=0)\n",
    "        #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        #print(\"sample_index\", sample_index)\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "        #print(sampled_token)\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_restricted.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        #sampled_token = index_lookup[sampled_token]\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #print(seed_text)\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "    #print(full_text)\n",
    "    #print(seed_text)\n",
    "\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **III**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification of **II**: just one word is a label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*InvalidArgumentError: Graph execution error:*\n",
    "\n",
    "*Input to reshape is a tensor with 320 values, but the requested shape has 8\n",
    "\t [[{{node ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_bool_Reshape}}]] [Op:__inference_train_function_2524]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 MB of Polish fairytales and stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reading files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing chapter names\n",
    "def remove_chapter_names(input_string, regex_string):\n",
    "    a1 = input_string\n",
    "    a2 = re.sub(rf'{regex_string}', '', a1)\n",
    "    return a2\n",
    "\n",
    "# removing footnotes enclosed in square brackets\n",
    "def remove_footnotes(input_string):\n",
    "    a1 = input_string\n",
    "    a2 = re.sub(r'\\[[\\d]*\\]', '', a1)\n",
    "    a2 = re.sub(r' \\[[^]]*\\]', '', a2)\n",
    "    return a2\n",
    "\n",
    "# dividing the punctuation with space from the words and removing the double spaces\n",
    "# def divide_punctuation(input_string, punctuation_to_tokenize):\n",
    "#     a1 = input_string\n",
    "#     a2 = re.sub(r'(['+punctuation_to_tokenize+'])', r' \\1 ', a1)\n",
    "#     a2 = re.sub(r'  ', r' ', a2)\n",
    "#     return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning chapter names, footnotes, no punctuation needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 98829 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences_fairy = []\n",
    "\n",
    "for file in glob.glob(\"Korpusy do bajek/*\"):\n",
    "\n",
    "    try:\n",
    "        #read the file\n",
    "        myfile = open(file,\"r\")\n",
    "        text = myfile.read()\n",
    "        myfile.close()\n",
    "\n",
    "        #clean chapter names\n",
    "        text = remove_chapter_names(text, 'ROZDZIAŁ[^\\n]+')\n",
    "        text = remove_chapter_names(text, 'Rozdział[^\\n]+')\n",
    "\n",
    "        #clean footnotes in square brackets\n",
    "        text = remove_footnotes(text)\n",
    "\n",
    "        #lower\n",
    "        text = text.lower()\n",
    "\n",
    "        #split to sentences\n",
    "        text = sent_tokenize(text)\n",
    "        #print(\"file \", file, \" generated \", len(text), \" words\")\n",
    "        \n",
    "        sentences_fairy.extend(text)\n",
    "    except:\n",
    "       continue\n",
    "    \n",
    "print(\"We have\", len(sentences_fairy), \"sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text consists of 1302994 words.\n"
     ]
    }
   ],
   "source": [
    "continuous_corpus_fairy = \" \".join(sentences_fairy)\n",
    "print(\"Full text consists of\", len(continuous_corpus_fairy.replace('\\n', ' ').split(' ')), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- tuf – sapnął pociąg, oznajmiając wszem i wobec wszystkim spóźnialskim, że nadeszła ostatnia chwila aby wskoczyć do swojego przedziału i odjechać w siną dal\\n\\nlokomotywa ospale ruszyła, pociągając za sobą powoli doczepione wagony\\n\\nsiedmioletnia mania i dziesięcioletni jurek jechali na swoje pierwsze wakacje bez mamy i taty.',\n",
       " 'oczywiście do babci jadzi eskortował ich dziadek tadek, ale fajny dziadek to nie to samo co strofująca swoje dzieci co chwilę mama\\n\\n- tato tylko uważaj na nie!',\n",
       " '– krzyknęła mama, żegnająca całą trójkę z perony\\n\\nmania z jurkiem, wychyleni przez otwarte okno machali mamie, dopóki jej czerwona bluzka nie znikła im w oddali\\n\\nz początku rodzeństwo zachwycone nowym środkiem transportu siedziało nawet spokojnie, dziadek tadek zamknął okno w przedziale, obciągnął blezer na swoim wydatnym brzuchu, wsadził na nos okulary i oddał się swojej ulubionej lekturze, działu sportowego.',\n",
       " 'pociąg stukał, pukał, pochylał się na nierównościach, wagon pachniał dziwnie, a jurka pochłonęło rozpracowywanie konstrukcji podłokietników.',\n",
       " 'mania przykleiła nos do szyby, za oknem mignęły jej ostatnie budynki i już po chwili wagony postukując wesoło mknęły wśród pól i łąk.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_fairy[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sentences lengths analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are of length 1 to 189\n"
     ]
    }
   ],
   "source": [
    "lens_fairy = []\n",
    "for sentence in sentences_fairy:\n",
    "  lens_fairy.append(len(sentence.replace('\\n', ' ').split(' ')))\n",
    "\n",
    "print(\"Sentences are of length\", min(lens_fairy), \"to\", max(lens_fairy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles:\n",
      "0.15 is 4 \n",
      "0.5 is 11 \n",
      "0.8 is 20 \n",
      "0.9 is 26 \n",
      "0.95 is 32\n",
      "Let's remove the sentences longer than 32.\n"
     ]
    }
   ],
   "source": [
    "#quantiles - 90% of sequences consists of at most 26 words, at most 15% is of length 4 or less\n",
    "lens_fairy.sort()\n",
    "print(\"Quantiles:\\n0.15 is\", lens_fairy[int(0.15*len(lens_fairy))],\n",
    " \"\\n0.5 is\", lens_fairy[int(0.5*len(lens_fairy))], \n",
    " \"\\n0.8 is\", lens_fairy[int(0.8*len(lens_fairy))], \n",
    " \"\\n0.9 is\", lens_fairy[int(0.9*len(lens_fairy))],\n",
    " \"\\n0.95 is\", lens_fairy[int(0.95*len(lens_fairy))])\n",
    "print(\"Let's remove the sentences longer than 32.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing long sentences\n",
    "#lowering the letters\n",
    "#removing new line signs\n",
    "\n",
    "sentences_short_fairy = []\n",
    "for sentence in sentences_fairy:\n",
    "  if not len(sentence.replace('\\n', ' ').split(' ')) > 32:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('\\n', ' ')\n",
    "    sentence = sentence.replace('—', '-')\n",
    "    sentences_short_fairy.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short sentences are of length 1 to 32\n"
     ]
    }
   ],
   "source": [
    "lens2_fairy = []\n",
    "for sentence in sentences_short_fairy:\n",
    "  lens2_fairy.append(len(sentence.replace('\\n', ' ').split(' ')))\n",
    "\n",
    "print(\"Short sentences are of length\", min(lens2_fairy), \"to\", max(lens2_fairy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oczywiście do babci jadzi eskortował ich dziadek tadek, ale fajny dziadek to nie to samo co strofująca swoje dzieci co chwilę mama  - tato tylko uważaj na nie!',\n",
       " 'pociąg stukał, pukał, pochylał się na nierównościach, wagon pachniał dziwnie, a jurka pochłonęło rozpracowywanie konstrukcji podłokietników.',\n",
       " 'mania przykleiła nos do szyby, za oknem mignęły jej ostatnie budynki i już po chwili wagony postukując wesoło mknęły wśród pól i łąk.',\n",
       " 'później pociąg wjechał do lasu i zwolnił.',\n",
       " 'teraz to i jurek przykleił się do szyby  jedynie pucia, czarna kudłata suczka rasy spaniel, towarzysząca rodzeństwu, nie wykazywał zainteresowania krajobrazem ani czymkolwiek innym.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_short_fairy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93937"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_short_fairy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenization. No punctuation included**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words: 91617\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Tokenizer on the Corpus\n",
    "tokenizer_fairy = Tokenizer(num_words=60000)\n",
    "tokenizer_fairy.fit_on_texts(sentences_short_fairy)\n",
    "\n",
    "# Vocabulary count of the corpus\n",
    "total_words_fairy = len(tokenizer_fairy.word_index)\n",
    "\n",
    "print(\"Total Unique Words:\", total_words_fairy)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the text into embeddings\n",
    "input_sequences_fairy = []\n",
    "for sentence in sentences_short_fairy:\n",
    "    token_list = tokenizer_fairy.texts_to_sequences([sentence])[0]\n",
    "    input_sequences_fairy.append(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Padding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_fairy = max(lens2_fairy)\n",
    "\n",
    "input_sequences_fairy = np.array(pad_sequences(input_sequences_fairy, maxlen=maxlen_fairy+1, padding='pre'))  #maxlen +1\n",
    "\n",
    "# predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "# #label = ku.to_categorical(label1, num_classes=total_words+1)\n",
    "\n",
    "maxlen_fairy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tensorflow Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset_fairy = tf.data.Dataset.from_tensor_slices(input_sequences_fairy)\n",
    "train_dataset_fairy = train_dataset_fairy.shuffle(buffer_size=256)\n",
    "train_dataset_fairy = train_dataset_fairy.batch(batch_size)\n",
    "type(train_dataset_fairy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    print(text.shape)\n",
    "    predictors, labels = text[:, :-1], text[:, 1:]    #offset by one + label is long!\n",
    "    print(predictors.shape, labels.shape)\n",
    "    return predictors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 33, 1)\n",
      "(None, 32, 1) (None, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_fairy = train_dataset_fairy.map(preprocessing)\n",
    "train_dataset_fairy = train_dataset_fairy.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The last successful model (previous dataset, attempt II).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transf_input (InputLayer)   [(None, 32)]              0         \n",
      "                                                                 \n",
      " transf_embed (TokenAndPosit  (None, 32, 32)           2932768   \n",
      " ionEmbedding)                                                   \n",
      "                                                                 \n",
      " transf_decod (TransformerDe  (None, 32, 32)           6464      \n",
      " coder)                                                          \n",
      "                                                                 \n",
      " transf_dense (Dense)        (None, 32, 91617)         3023361   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,962,593\n",
      "Trainable params: 5,962,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  #inicially 128\n",
    "num_heads = 4\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(maxlen_fairy, ), dtype=tf.int32, name='transf_input')\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(total_words_fairy, maxlen_fairy, embed_dim, name='transf_embed')(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5, \n",
    "                                                            name='transf_decod')(embedding_layer)\n",
    "    outputs = keras.layers.Dense(total_words_fairy, activation='softmax', name='transf_dense')(decoder)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_transf_fairy = create_model()\n",
    "model_transf_fairy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Quick training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parameters: batch_size = 8, epochs = 5, embed_dim = 32, num_heads = 4, Adam optimizer lr = 0.0001 -> generate nans as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parameters: batch_size = 4, epochs = 5, embed_dim = 32, num_heads = 4, Adam optimizer lr = 0.0001 -> generate nans as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parameters: batch_size = 4, epochs = 5, embed_dim = 32, num_heads = 1, Adam optimizer lr = 0.0001 -> generate nans as weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11743/11743 [==============================] - 603s 51ms/step - loss: 3.5383 - perplexity: 34.4086 - accuracy: 0.6660\n",
      "Epoch 2/5\n",
      "11743/11743 [==============================] - 598s 51ms/step - loss: 2.7942 - perplexity: 16.3493 - accuracy: 0.6780\n",
      "Epoch 3/5\n",
      "11743/11743 [==============================] - 596s 51ms/step - loss: 2.6982 - perplexity: 14.8532 - accuracy: 0.6850\n",
      "Epoch 4/5\n",
      "11743/11743 [==============================] - 596s 51ms/step - loss: 2.6234 - perplexity: 13.7831 - accuracy: 0.6894\n",
      "Epoch 5/5\n",
      "11743/11743 [==============================] - 599s 51ms/step - loss: 2.5607 - perplexity: 12.9453 - accuracy: 0.6926\n"
     ]
    }
   ],
   "source": [
    "history_fairy = model_transf_fairy.fit(train_dataset_fairy, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_transf_fairy.weights)):\n",
    "    model_transf_fairy.weights[i]._handle_name = model_transf_fairy.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_transf_fairy.optimizer.weights)):\n",
    "    model_transf_fairy.optimizer.weights[i]._handle_name = model_transf_fairy.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_transf_fairy.save(\"transformer_fairy.keras\")\n",
    "loaded_model_transf_fairy = tf.keras.models.load_model(\"transformer_fairy.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "    #print(\"logits shape: \", logits.shape)\n",
    "    logits, indices = tf.math.top_k(logits, k=15, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami a kapitan z co nie z to w na o co kapitan ale nami sobą\n",
      "nawet tej chciał co ten na to w kilku chwila to nocy się kapitan o gdy po czy\n",
      "ale ale czy w tak kapitan ale z o tylko w to chwili nie może jak to jego jego po\n",
      "nie było nas jestem kapitan po co z gdy na nie co co w nie ale ale tak już o\n",
      "drodze powodu chwilę się ciągu kapitan to co ten pod nie nam wodą statku po o po\n",
      "ale w nie co w a czy w kapitan o miał która kilku prostu tej tu już górę tej do\n",
      "że jest i góry w kapitanie a a gdy to to kapitan nie ale a tak tak na na do nocy\n",
      "co go jeszcze widziałem on o nim bo już w nie ja końcu są\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy = max(lens2_fairy)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy.predict(test_sequence, verbose=0)\n",
    "        #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        #print(\"sample_index\", sample_index)\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "        #print(sampled_token)\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        #sampled_token = index_lookup[sampled_token]\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #print(seed_text)\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "    #print(full_text)\n",
    "    #print(seed_text)\n",
    "\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **More training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy = tf.keras.models.load_model(\"transformer_fairy.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11743/11743 [==============================] - 596s 51ms/step - loss: 2.5074 - perplexity: 12.2729 - accuracy: 0.6953\n",
      "Epoch 2/10\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.4624 - perplexity: 11.7325 - accuracy: 0.6976\n",
      "Epoch 3/10\n",
      "11743/11743 [==============================] - 495s 42ms/step - loss: 2.4248 - perplexity: 11.2998 - accuracy: 0.6996\n",
      "Epoch 4/10\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.3936 - perplexity: 10.9533 - accuracy: 0.7014\n",
      "Epoch 5/10\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.3679 - perplexity: 10.6751 - accuracy: 0.7030\n",
      "Epoch 6/10\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.3466 - perplexity: 10.4501 - accuracy: 0.7043\n",
      "Epoch 7/10\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.3293 - perplexity: 10.2707 - accuracy: 0.7055\n",
      "Epoch 8/10\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.3150 - perplexity: 10.1253 - accuracy: 0.7063\n",
      "Epoch 9/10\n",
      "11743/11743 [==============================] - 497s 42ms/step - loss: 2.3033 - perplexity: 10.0074 - accuracy: 0.7072\n",
      "Epoch 10/10\n",
      "11743/11743 [==============================] - 497s 42ms/step - loss: 2.2937 - perplexity: 9.9112 - accuracy: 0.7079\n"
     ]
    }
   ],
   "source": [
    "history2_fairy = loaded_model_transf_fairy.fit(train_dataset_fairy, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy.weights)):\n",
    "    loaded_model_transf_fairy.weights[i]._handle_name = loaded_model_transf_fairy.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy.optimizer.weights)):\n",
    "    loaded_model_transf_fairy.optimizer.weights[i]._handle_name = loaded_model_transf_fairy.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_fairy.save(\"transformer_fairy2.keras\")\n",
    "loaded_model_transf_fairy2 = tf.keras.models.load_model(\"transformer_fairy2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami zresztą po z ale w to kapitan tak a z co a czy każdym chwilę\n",
      "nie wody odległości nie to w jest bo przez miał tym nigdy słuszność ich zresztą\n",
      "zresztą kapitan co gdy nie co tak na ale nautilus nautilus kapitan że panie jak\n",
      "farragut w tylko powierzchnię tak nemo profesorze kapitan tylko się jak przez go\n",
      "po ale kapitan w nie co w nautilus z tak z ale conseil w kilka upływie żeby ma\n",
      "każdym tego nas który kroków by rokiem do przy na tym kapitan i i na kapitan a\n",
      "co i tak nautilus nautilus to tak tych jego kazał nautilus niż w łatwo był\n",
      "dobrze statek zdawał której od odpowiedział był mowgli kapitan tak ale w\n",
      "nautilus na na tak nautilus zresztą gdy a nie nas tak i po jest powierzchni w\n",
      "ned był długo pewnym morza landa tak się bardzo\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy = max(lens2_fairy)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy2.predict(test_sequence, verbose=0)\n",
    "        #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        #print(\"sample_index\", sample_index)\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "        #print(sampled_token)\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        #sampled_token = index_lookup[sampled_token]\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #print(seed_text)\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "    #print(full_text)\n",
    "    #print(seed_text)\n",
    "\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Even more training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy2 = tf.keras.models.load_model(\"transformer_fairy2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11743/11743 [==============================] - 489s 42ms/step - loss: 2.2857 - perplexity: 9.8329 - accuracy: 0.7083\n",
      "Epoch 2/10\n",
      "11743/11743 [==============================] - 489s 42ms/step - loss: 2.2795 - perplexity: 9.7717 - accuracy: 0.7088\n",
      "Epoch 3/10\n",
      "11743/11743 [==============================] - 488s 42ms/step - loss: 2.2741 - perplexity: 9.7195 - accuracy: 0.7092\n",
      "Epoch 4/10\n",
      "11743/11743 [==============================] - 487s 42ms/step - loss: 2.2698 - perplexity: 9.6776 - accuracy: 0.7095\n",
      "Epoch 5/10\n",
      "11743/11743 [==============================] - 489s 42ms/step - loss: 2.2662 - perplexity: 9.6424 - accuracy: 0.7098\n",
      "Epoch 6/10\n",
      "11743/11743 [==============================] - 490s 42ms/step - loss: 2.2632 - perplexity: 9.6142 - accuracy: 0.7100\n",
      "Epoch 7/10\n",
      "11743/11743 [==============================] - 492s 42ms/step - loss: 2.2609 - perplexity: 9.5920 - accuracy: 0.7103\n",
      "Epoch 8/10\n",
      "11743/11743 [==============================] - 492s 42ms/step - loss: 2.2588 - perplexity: 9.5717 - accuracy: 0.7104\n",
      "Epoch 9/10\n",
      "11743/11743 [==============================] - 498s 42ms/step - loss: 2.2572 - perplexity: 9.5560 - accuracy: 0.7105\n",
      "Epoch 10/10\n",
      "11743/11743 [==============================] - 506s 43ms/step - loss: 2.2559 - perplexity: 9.5440 - accuracy: 0.7107\n"
     ]
    }
   ],
   "source": [
    "history3_fairy = loaded_model_transf_fairy2.fit(train_dataset_fairy, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy2.weights)):\n",
    "    loaded_model_transf_fairy2.weights[i]._handle_name = loaded_model_transf_fairy2.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy2.optimizer.weights)):\n",
    "    loaded_model_transf_fairy2.optimizer.weights[i]._handle_name = loaded_model_transf_fairy2.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_fairy2.save(\"transformer_fairy3.keras\")\n",
    "loaded_model_transf_fairy3 = tf.keras.models.load_model(\"transformer_fairy3.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami po nie tak na kapitan tak gdy z nie a ale nie na nami to tym\n",
      "że nie jednak widziałem co jego tylko tak kapitan pan nie moi mogłem na nautilus\n",
      "w na kapitan po kapitan zresztą kapitan i nautilus co a pan towarzysze pokład\n",
      "tej nemo nie by zbliżył gdy moi rzeczy ma na także słuszność mówił nautilus gdy\n",
      "to i z i nautilus po czy nie tak ale nie to ani zdawał już jego na mi że miał\n",
      "tego po to są z jest powodu w ale nie zresztą tak zresztą to zresztą w nautilus\n",
      "na po nautilus za jak wodzie się że w tych powierzchni w w już której których\n",
      "tej było samej po to i po w zresztą ale kapitan to kapitan to z na się dla\n",
      "trzech ja ciągu pan na że powierzchnię których do się są te od dzieci\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy = max(lens2_fairy)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy3.predict(test_sequence, verbose=0)\n",
    "        #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        #print(\"sample_index\", sample_index)\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "        #print(sampled_token)\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        #sampled_token = index_lookup[sampled_token]\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #print(seed_text)\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "    #print(full_text)\n",
    "    #print(seed_text)\n",
    "\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **More more training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy3 = tf.keras.models.load_model(\"transformer_fairy3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "11743/11743 [==============================] - 497s 42ms/step - loss: 2.2546 - perplexity: 9.5315 - accuracy: 0.7109\n",
      "Epoch 2/15\n",
      "11743/11743 [==============================] - 502s 43ms/step - loss: 2.2537 - perplexity: 9.5227 - accuracy: 0.7110\n",
      "Epoch 3/15\n",
      "11743/11743 [==============================] - 509s 43ms/step - loss: 2.2528 - perplexity: 9.5142 - accuracy: 0.7110\n",
      "Epoch 4/15\n",
      "11743/11743 [==============================] - 497s 42ms/step - loss: 2.2523 - perplexity: 9.5100 - accuracy: 0.7111\n",
      "Epoch 5/15\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.2515 - perplexity: 9.5022 - accuracy: 0.7112\n",
      "Epoch 6/15\n",
      "11743/11743 [==============================] - 500s 43ms/step - loss: 2.2512 - perplexity: 9.4987 - accuracy: 0.7113\n",
      "Epoch 7/15\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.2507 - perplexity: 9.4947 - accuracy: 0.7114\n",
      "Epoch 8/15\n",
      "11743/11743 [==============================] - 500s 43ms/step - loss: 2.2504 - perplexity: 9.4913 - accuracy: 0.7115\n",
      "Epoch 9/15\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.2502 - perplexity: 9.4899 - accuracy: 0.7116\n",
      "Epoch 10/15\n",
      "11743/11743 [==============================] - 497s 42ms/step - loss: 2.2498 - perplexity: 9.4860 - accuracy: 0.7116\n",
      "Epoch 11/15\n",
      "11743/11743 [==============================] - 499s 42ms/step - loss: 2.2496 - perplexity: 9.4836 - accuracy: 0.7117\n",
      "Epoch 12/15\n",
      "11743/11743 [==============================] - 497s 42ms/step - loss: 2.2493 - perplexity: 9.4815 - accuracy: 0.7118\n",
      "Epoch 13/15\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.2492 - perplexity: 9.4805 - accuracy: 0.7118\n",
      "Epoch 14/15\n",
      "11743/11743 [==============================] - 497s 42ms/step - loss: 2.2493 - perplexity: 9.4807 - accuracy: 0.7119\n",
      "Epoch 15/15\n",
      "11743/11743 [==============================] - 498s 42ms/step - loss: 2.2488 - perplexity: 9.4762 - accuracy: 0.7120\n"
     ]
    }
   ],
   "source": [
    "history4_fairy = loaded_model_transf_fairy3.fit(train_dataset_fairy, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy3.weights)):\n",
    "    loaded_model_transf_fairy3.weights[i]._handle_name = loaded_model_transf_fairy3.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "for i in range(len(loaded_model_transf_fairy3.optimizer.weights)):\n",
    "    loaded_model_transf_fairy3.optimizer.weights[i]._handle_name = loaded_model_transf_fairy3.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_fairy3.save(\"transformer_fairy4.keras\")\n",
    "loaded_model_transf_fairy4 = tf.keras.models.load_model(\"transformer_fairy4.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami po w i po tak na kapitan i kapitan i nautilus czy nie mną\n",
      "chwilę chwili pod bardzo kapitan wziął z się potem morze czy sobą zaczął to\n",
      "wołać gdy kapitan zresztą w z po to tak tak a po ale zresztą a pan mnie w\n",
      "których co że tym na chcesz każdym by względem go ich więcej kapitan zresztą gdy\n",
      "z zresztą gdy nautilus po to gdy nie nie nautilus jeżeli było wziął w jest stał\n",
      "jeszcze ma na tak takim przy słuszność mi ich głowę kapitan w w z i tak w\n",
      "nautilus zresztą na na kapitan kapitan i i nemo każdym z tym panie powierzchnię\n",
      "kapitana w razie miejscu morza tych jego zwierząt gdy nie na a i i nautilus ale\n",
      "gdy i nautilus z co po a nagle tym się był w stanął w może którzy bardzo nagle\n",
      "na nie statek\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy = max(lens2_fairy)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy4.predict(test_sequence, verbose=0)\n",
    "        #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        #print(\"sample_index\", sample_index)\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "        #print(sampled_token)\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        #sampled_token = index_lookup[sampled_token]\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #print(seed_text)\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "    #print(full_text)\n",
    "    #print(seed_text)\n",
    "\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check out more training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy4 = tf.keras.models.load_model(\"transformer_fairy4.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "11743/11743 [==============================] - 501s 43ms/step - loss: 2.2486 - perplexity: 9.4746 - accuracy: 0.7120\n",
      "Epoch 2/15\n",
      "11743/11743 [==============================] - 509s 43ms/step - loss: 2.2483 - perplexity: 9.4716 - accuracy: 0.7121\n",
      "Epoch 3/15\n",
      "11743/11743 [==============================] - 496s 42ms/step - loss: 2.2477 - perplexity: 9.4663 - accuracy: 0.7122\n",
      "Epoch 4/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2478 - perplexity: 9.4670 - accuracy: 0.7121\n",
      "Epoch 5/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2475 - perplexity: 9.4637 - accuracy: 0.7123\n",
      "Epoch 6/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2468 - perplexity: 9.4578 - accuracy: 0.7124\n",
      "Epoch 7/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2466 - perplexity: 9.4553 - accuracy: 0.7124\n",
      "Epoch 8/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2463 - perplexity: 9.4529 - accuracy: 0.7124\n",
      "Epoch 9/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2459 - perplexity: 9.4488 - accuracy: 0.7125\n",
      "Epoch 10/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2455 - perplexity: 9.4453 - accuracy: 0.7125\n",
      "Epoch 11/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2454 - perplexity: 9.4442 - accuracy: 0.7125\n",
      "Epoch 12/15\n",
      "11743/11743 [==============================] - 494s 42ms/step - loss: 2.2448 - perplexity: 9.4389 - accuracy: 0.7126\n",
      "Epoch 13/15\n",
      "11743/11743 [==============================] - 493s 42ms/step - loss: 2.2446 - perplexity: 9.4366 - accuracy: 0.7126\n",
      "Epoch 14/15\n",
      "11743/11743 [==============================] - 494s 42ms/step - loss: 2.2442 - perplexity: 9.4331 - accuracy: 0.7127\n",
      "Epoch 15/15\n",
      "11743/11743 [==============================] - 494s 42ms/step - loss: 2.2440 - perplexity: 9.4314 - accuracy: 0.7127\n"
     ]
    }
   ],
   "source": [
    "history5_fairy = loaded_model_transf_fairy4.fit(train_dataset_fairy, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy4.weights)):\n",
    "    loaded_model_transf_fairy4.weights[i]._handle_name = loaded_model_transf_fairy4.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "for i in range(len(loaded_model_transf_fairy4.optimizer.weights)):\n",
    "    loaded_model_transf_fairy4.optimizer.weights[i]._handle_name = loaded_model_transf_fairy4.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy4.save(\"transformer_fairy5.keras\")\n",
    "loaded_model_transf_fairy5 = tf.keras.models.load_model(\"transformer_fairy5.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami na zresztą nautilus w i zresztą i z w i z z w mało nimi jego\n",
      "zbliżył że ówdzie ten tej każdym to mówił luźne ziemi na te platformę po na na\n",
      "po nie tak się tak zresztą tak tak kapitan tak nie same drugiej godzinę jest jak\n",
      "co jak na wspomnienia potem mi mi ich je wyrazu gdy ale i z się po nie nie tak\n",
      "na tak w i ją pod tylko w że można się łatwo nie wieczór tym było było zapadł na\n",
      "ojciec i na nie to się i nautilus tak po nie zresztą nie i na to nie miał że\n",
      "zbliżył południu od że że co tylko nas z w powodu lecz ale kapitan to i a gdy\n",
      "tak ale ale zresztą to to i każdym nautilus tych na w ned co się samym którzy\n",
      "ciągu do o chwili sto\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy = max(lens2_fairy)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy5.predict(test_sequence, verbose=0)\n",
    "        #print(\"Softmax predictions shape:\", soft_pred.shape)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        #print(\"sample_index\", sample_index)\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "        #print(sampled_token)\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        #sampled_token = index_lookup[sampled_token]\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #print(seed_text)\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "    #print(full_text)\n",
    "    #print(seed_text)\n",
    "\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 MB of simple Polish fairytale set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple and small Polish dataset with restricted difficult vocabulary, based on tales suitable for children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reading files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 62031 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences_fairy_simple = []\n",
    "\n",
    "for file in glob.glob(\"Prosty korpus bajkowy/*\"):\n",
    "\n",
    "    try:\n",
    "        #read the file\n",
    "        myfile = open(file,\"r\")\n",
    "        text = myfile.read()\n",
    "        myfile.close()\n",
    "\n",
    "        #lower\n",
    "        text = text.lower()\n",
    "\n",
    "        #split to sentences\n",
    "        text = sent_tokenize(text)\n",
    "        #print(\"file \", file, \" generated \", len(text), \" words\")\n",
    "        \n",
    "        sentences_fairy_simple.extend(text)\n",
    "    except:\n",
    "       continue\n",
    "    \n",
    "print(\"We have\", len(sentences_fairy_simple), \"sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text consists of 866051 words.\n"
     ]
    }
   ],
   "source": [
    "continuous_corpus_fairy_simple = \" \".join(sentences_fairy_simple)\n",
    "print(\"Full text consists of\", len(continuous_corpus_fairy_simple.replace('\\n', ' ').split(' ')), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nazywam się sindbad.',\n",
       " 'mieszkam stale w bagdadzie.',\n",
       " 'rodzice moi, umierając, zostawili mi w spadku tysiąc worów złota, tysiąc beczek srebra, sto pałaców, sto ogrodów i jeden trzonowy ząb mego pradziadka, który ojciec mój przechowywał w hebanowej szkatułce, jako pamiątkę i osobliwość.',\n",
       " 'pradziadek mój przez całe życie chorował na ból zębów i co pewien czas inny ząb musiał wyrywać, tak że w końcu jeden mu tylko ząb trzonowy pozostał.',\n",
       " 'umierając, kazał sobie wyrwać i ten ostatni ząb trzonowy, który przeszedł w spadku od mego dziada do mego ojca, a od ojca — do mnie.']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_fairy_simple[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sentences lengths analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are of length 1 to 163\n"
     ]
    }
   ],
   "source": [
    "lens_fairy_simple = []\n",
    "for sentence in sentences_fairy_simple:\n",
    "  lens_fairy_simple.append(len(sentence.replace('\\n', ' ').split(' ')))\n",
    "\n",
    "print(\"Sentences are of length\", min(lens_fairy_simple), \"to\", max(lens_fairy_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles:\n",
      "0.15 is 4 \n",
      "0.5 is 11 \n",
      "0.8 is 21 \n",
      "0.9 is 28 \n",
      "0.95 is 35\n",
      "Let's remove the sentences longer than 35.\n"
     ]
    }
   ],
   "source": [
    "#quantiles - 90% of sequences consists of at most 26 words, at most 15% is of length 4 or less\n",
    "lens_fairy_simple.sort()\n",
    "print(\"Quantiles:\\n0.15 is\", lens_fairy_simple[int(0.15*len(lens_fairy_simple))],\n",
    " \"\\n0.5 is\", lens_fairy_simple[int(0.5*len(lens_fairy_simple))], \n",
    " \"\\n0.8 is\", lens_fairy_simple[int(0.8*len(lens_fairy_simple))], \n",
    " \"\\n0.9 is\", lens_fairy_simple[int(0.9*len(lens_fairy_simple))],\n",
    " \"\\n0.95 is\", lens_fairy_simple[int(0.95*len(lens_fairy_simple))])\n",
    "print(\"Let's remove the sentences longer than 35.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing long sentences\n",
    "#lowering the letters\n",
    "#removing new line signs\n",
    "\n",
    "sentences_short_fairy_simple = []\n",
    "for sentence in sentences_fairy_simple:\n",
    "  if not len(sentence.replace('\\n', ' ').split(' ')) > 35:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('\\n', ' ')\n",
    "    sentence = sentence.replace('—', '-')\n",
    "    sentences_short_fairy_simple.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short sentences are of length 1 to 35\n"
     ]
    }
   ],
   "source": [
    "lens2_fairy_simple = []\n",
    "for sentence in sentences_short_fairy_simple:\n",
    "  lens2_fairy_simple.append(len(sentence.replace('\\n', ' ').split(' ')))\n",
    "\n",
    "print(\"Short sentences are of length\", min(lens2_fairy_simple), \"to\", max(lens2_fairy_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59249"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_short_fairy_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenization. No punctuation included**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words: 72795\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Tokenizer on the Corpus\n",
    "tokenizer_fairy_simple = Tokenizer(num_words=70000)\n",
    "tokenizer_fairy_simple.fit_on_texts(sentences_short_fairy_simple)\n",
    "\n",
    "# Vocabulary count of the corpus\n",
    "total_words_fairy_simple = len(tokenizer_fairy_simple.word_index)\n",
    "\n",
    "print(\"Total Unique Words:\", total_words_fairy_simple)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the text into embeddings\n",
    "input_sequences_fairy_simple = []\n",
    "for sentence in sentences_short_fairy_simple:\n",
    "    token_list = tokenizer_fairy_simple.texts_to_sequences([sentence])[0]\n",
    "    input_sequences_fairy_simple.append(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_fairy_simple = max(lens2_fairy_simple)\n",
    "input_sequences_fairy_simple = np.array(pad_sequences(input_sequences_fairy_simple, maxlen=maxlen_fairy_simple+1, padding='pre'))  #maxlen +1\n",
    "maxlen_fairy_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tensorflow Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset_fairy_simple = tf.data.Dataset.from_tensor_slices(input_sequences_fairy_simple)\n",
    "train_dataset_fairy_simple = train_dataset_fairy_simple.shuffle(buffer_size=256)\n",
    "train_dataset_fairy_simple = train_dataset_fairy_simple.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    print(text.shape)\n",
    "    predictors, labels = text[:, :-1], text[:, 1:]    #offset by one + label is long!\n",
    "    print(predictors.shape, labels.shape)\n",
    "    return predictors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 36, 1)\n",
      "(None, 35, 1) (None, 35, 1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_fairy_simple = train_dataset_fairy_simple.map(preprocessing)\n",
    "train_dataset_fairy_simple = train_dataset_fairy_simple.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same model on a simpler texts, less words (but almost all included; 72975 -> 70000). Let's use more heads to make the decision algorithm more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transf_input (InputLayer)   [(None, 35)]              0         \n",
      "                                                                 \n",
      " transf_embed (TokenAndPosit  (None, 35, 32)           2330560   \n",
      " ionEmbedding)                                                   \n",
      "                                                                 \n",
      " transf_decod (TransformerDe  (None, 35, 32)           6464      \n",
      " coder)                                                          \n",
      "                                                                 \n",
      " transf_dense (Dense)        (None, 35, 72795)         2402235   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,739,259\n",
      "Trainable params: 4,739,259\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  #inicially 128\n",
    "num_heads = 8\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(maxlen_fairy_simple, ), dtype=tf.int32, name='transf_input')\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(total_words_fairy_simple, maxlen_fairy_simple, embed_dim, name='transf_embed')(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5, \n",
    "                                                            name='transf_decod')(embedding_layer)\n",
    "    outputs = keras.layers.Dense(total_words_fairy_simple, activation='softmax', name='transf_dense')(decoder)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_transf_fairy_simple = create_model()\n",
    "model_transf_fairy_simple.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7407/7407 [==============================] - 265s 36ms/step - loss: 3.8736 - perplexity: 48.1132 - accuracy: 0.6622\n",
      "Epoch 2/10\n",
      "7407/7407 [==============================] - 265s 36ms/step - loss: 2.8497 - perplexity: 17.2827 - accuracy: 0.6743\n",
      "Epoch 3/10\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.7556 - perplexity: 15.7309 - accuracy: 0.6800\n",
      "Epoch 4/10\n",
      "7407/7407 [==============================] - 266s 36ms/step - loss: 2.6750 - perplexity: 14.5123 - accuracy: 0.6856\n",
      "Epoch 5/10\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.6038 - perplexity: 13.5148 - accuracy: 0.6893\n",
      "Epoch 6/10\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.5391 - perplexity: 12.6685 - accuracy: 0.6924\n",
      "Epoch 7/10\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.4800 - perplexity: 11.9408 - accuracy: 0.6955\n",
      "Epoch 8/10\n",
      "7407/7407 [==============================] - 265s 36ms/step - loss: 2.4268 - perplexity: 11.3231 - accuracy: 0.6984\n",
      "Epoch 9/10\n",
      "7407/7407 [==============================] - 266s 36ms/step - loss: 2.3795 - perplexity: 10.7991 - accuracy: 0.7011\n",
      "Epoch 10/10\n",
      "7407/7407 [==============================] - 265s 36ms/step - loss: 2.3379 - perplexity: 10.3589 - accuracy: 0.7036\n"
     ]
    }
   ],
   "source": [
    "history_fairy_simple = model_transf_fairy_simple.fit(train_dataset_fairy_simple, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_transf_fairy_simple.weights)):\n",
    "    model_transf_fairy_simple.weights[i]._handle_name = model_transf_fairy_simple.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "for i in range(len(model_transf_fairy_simple.optimizer.weights)):\n",
    "    model_transf_fairy_simple.optimizer.weights[i]._handle_name = model_transf_fairy_simple.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_transf_fairy_simple.save(\"transformer_fairy_simple.keras\")\n",
    "model_transf_fairy_simple2 = tf.keras.models.load_model(\"transformer_fairy_simple.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami o następnie nie co w w ale ale po o na pewnego kiedy po gdy\n",
      "mógł aż w ci górę tak ile pewnego śmierci się jego w mi dla miejsce nie może i w\n",
      "kiedy i w pewnego gdy kiedy o jestem i kiedy na w ale nie już jej poszedł dawna\n",
      "ją co mu górę jestem się i tu na w nie wodzie o gdy na po po o i kiedy w pewnego\n",
      "w gdy król ale to domu a z kilku tym król jednak się teraz z powrotem celu tak\n",
      "co do wielkie mnie to na ale to gdy i gdy tak król w o nie na kiedy i lasu po\n",
      "koniec jest dlatego mi nim mógł znów i nie żeby za ze było chwilę bardziej po\n",
      "pewnego nie jestem kiedy kiedy i kiedy kiedy jestem na z o gdy tak dla i jednego\n",
      "i już ją i nich zaczął swoich i nigdy zapytał się nie głosem mam\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy_simple-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy_simple.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy_simple = max(lens2_fairy_simple)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy_simple+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = model_transf_fairy_simple2.predict(test_sequence, verbose=0)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy_simple.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **More training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy_simple2 = tf.keras.models.load_model(\"transformer_fairy_simple.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7407/7407 [==============================] - 265s 36ms/step - loss: 2.3014 - perplexity: 9.9877 - accuracy: 0.7061\n",
      "Epoch 2/45\n",
      "7407/7407 [==============================] - 267s 36ms/step - loss: 2.2697 - perplexity: 9.6766 - accuracy: 0.7083\n",
      "Epoch 3/45\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.2421 - perplexity: 9.4126 - accuracy: 0.7103\n",
      "Epoch 4/45\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.2178 - perplexity: 9.1870 - accuracy: 0.7121\n",
      "Epoch 5/45\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.1969 - perplexity: 8.9970 - accuracy: 0.7137\n",
      "Epoch 6/45\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.1787 - perplexity: 8.8351 - accuracy: 0.7151\n",
      "Epoch 7/45\n",
      "7407/7407 [==============================] - 270s 37ms/step - loss: 2.1631 - perplexity: 8.6984 - accuracy: 0.7162\n",
      "Epoch 8/45\n",
      "7407/7407 [==============================] - 270s 36ms/step - loss: 2.1494 - perplexity: 8.5801 - accuracy: 0.7170\n",
      "Epoch 9/45\n",
      "7407/7407 [==============================] - 274s 37ms/step - loss: 2.1375 - perplexity: 8.4784 - accuracy: 0.7179\n",
      "Epoch 10/45\n",
      "7407/7407 [==============================] - 274s 37ms/step - loss: 2.1276 - perplexity: 8.3946 - accuracy: 0.7186\n",
      "Epoch 11/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.1186 - perplexity: 8.3197 - accuracy: 0.7191\n",
      "Epoch 12/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.1108 - perplexity: 8.2544 - accuracy: 0.7195\n",
      "Epoch 13/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.1042 - perplexity: 8.2004 - accuracy: 0.7199\n",
      "Epoch 14/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0985 - perplexity: 8.1541 - accuracy: 0.7204\n",
      "Epoch 15/45\n",
      "7407/7407 [==============================] - 270s 36ms/step - loss: 2.0938 - perplexity: 8.1155 - accuracy: 0.7206\n",
      "Epoch 16/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0890 - perplexity: 8.0770 - accuracy: 0.7208\n",
      "Epoch 17/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0855 - perplexity: 8.0484 - accuracy: 0.7211\n",
      "Epoch 18/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0822 - perplexity: 8.0225 - accuracy: 0.7213\n",
      "Epoch 19/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0796 - perplexity: 8.0012 - accuracy: 0.7213\n",
      "Epoch 20/45\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0769 - perplexity: 7.9795 - accuracy: 0.7216\n",
      "Epoch 21/45\n",
      "7407/7407 [==============================] - 267s 36ms/step - loss: 2.0751 - perplexity: 7.9650 - accuracy: 0.7217\n",
      "Epoch 22/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0734 - perplexity: 7.9519 - accuracy: 0.7217\n",
      "Epoch 23/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0719 - perplexity: 7.9396 - accuracy: 0.7219\n",
      "Epoch 24/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0709 - perplexity: 7.9320 - accuracy: 0.7219\n",
      "Epoch 25/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0697 - perplexity: 7.9227 - accuracy: 0.7219\n",
      "Epoch 26/45\n",
      "7407/7407 [==============================] - 267s 36ms/step - loss: 2.0691 - perplexity: 7.9179 - accuracy: 0.7220\n",
      "Epoch 27/45\n",
      "7407/7407 [==============================] - 266s 36ms/step - loss: 2.0682 - perplexity: 7.9103 - accuracy: 0.7221\n",
      "Epoch 28/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0678 - perplexity: 7.9074 - accuracy: 0.7220\n",
      "Epoch 29/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0676 - perplexity: 7.9055 - accuracy: 0.7222\n",
      "Epoch 30/45\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0669 - perplexity: 7.8999 - accuracy: 0.7222\n",
      "Epoch 31/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0667 - perplexity: 7.8985 - accuracy: 0.7222\n",
      "Epoch 32/45\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0662 - perplexity: 7.8947 - accuracy: 0.7223\n",
      "Epoch 33/45\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0662 - perplexity: 7.8949 - accuracy: 0.7223\n",
      "Epoch 34/45\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0658 - perplexity: 7.8917 - accuracy: 0.7222\n",
      "Epoch 35/45\n",
      "7407/7407 [==============================] - 265s 36ms/step - loss: 2.0660 - perplexity: 7.8934 - accuracy: 0.7223\n",
      "Epoch 36/45\n",
      "7407/7407 [==============================] - 264s 36ms/step - loss: 2.0658 - perplexity: 7.8918 - accuracy: 0.7223\n",
      "Epoch 37/45\n",
      "7407/7407 [==============================] - 264s 36ms/step - loss: 2.0659 - perplexity: 7.8924 - accuracy: 0.7224\n",
      "Epoch 38/45\n",
      "7407/7407 [==============================] - 263s 36ms/step - loss: 2.0658 - perplexity: 7.8917 - accuracy: 0.7223\n",
      "Epoch 39/45\n",
      "7407/7407 [==============================] - 264s 36ms/step - loss: 2.0657 - perplexity: 7.8910 - accuracy: 0.7223\n",
      "Epoch 40/45\n",
      "7407/7407 [==============================] - 264s 36ms/step - loss: 2.0658 - perplexity: 7.8913 - accuracy: 0.7223\n",
      "Epoch 41/45\n",
      "7407/7407 [==============================] - 263s 36ms/step - loss: 2.0660 - perplexity: 7.8935 - accuracy: 0.7224\n",
      "Epoch 42/45\n",
      "7407/7407 [==============================] - 263s 36ms/step - loss: 2.0658 - perplexity: 7.8914 - accuracy: 0.7224\n",
      "Epoch 43/45\n",
      "7407/7407 [==============================] - 263s 36ms/step - loss: 2.0661 - perplexity: 7.8938 - accuracy: 0.7223\n",
      "Epoch 44/45\n",
      "7407/7407 [==============================] - 264s 36ms/step - loss: 2.0661 - perplexity: 7.8940 - accuracy: 0.7224\n",
      "Epoch 45/45\n",
      "7407/7407 [==============================] - 264s 36ms/step - loss: 2.0661 - perplexity: 7.8941 - accuracy: 0.7224\n"
     ]
    }
   ],
   "source": [
    "history2_fairy_simple = loaded_model_transf_fairy_simple2.fit(train_dataset_fairy_simple, epochs=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy_simple2.weights)):\n",
    "    loaded_model_transf_fairy_simple2.weights[i]._handle_name = loaded_model_transf_fairy_simple2.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "for i in range(len(loaded_model_transf_fairy_simple2.optimizer.weights)):\n",
    "    loaded_model_transf_fairy_simple2.optimizer.weights[i]._handle_name = loaded_model_transf_fairy_simple2.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_fairy_simple2.save(\"transformer_fairy_simple2.keras\")\n",
    "loaded_model_transf_fairy_simple3 = tf.keras.models.load_model(\"transformer_fairy_simple2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami ale a nie to o w z a na w gdy z to gdy ale za i na ty każdym\n",
      "ja pałacu jego nagle wiele ziemię słowem w i głos jej młodzieńca ale nie a w po\n",
      "pewnego to król nie to ale to król na gdy mój że mogła mgnieniu ranka posłał za\n",
      "już widok boże się lecz nią a za i rękę i to i ale ale król nie po kiedy po po\n",
      "nie ale to następnie mąż tak jest król kazał pewnym prostu jest znaczy nie że ci\n",
      "plecami że ma nie słowa po z i w nie ale w i w na o król król pewnego nie w ale\n",
      "niego tym w nie imię i czasu świecie noc którym i z i powiedział uśmiechu z i w\n",
      "w po z gdy kiedy w pewnego w kiedy na a i udał nie rzekł mgnieniu pewnością ją\n",
      "gdy z ona bo w że młodzieniec ma tym się samym\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy_simple-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy_simple.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy_simple = max(lens2_fairy_simple)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy_simple+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy_simple3.predict(test_sequence, verbose=0)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy_simple.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Even more training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy_simple3 = tf.keras.models.load_model(\"transformer_fairy_simple2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7407/7407 [==============================] - 265s 36ms/step - loss: 2.0662 - perplexity: 7.8946 - accuracy: 0.7224\n",
      "Epoch 2/25\n",
      "7407/7407 [==============================] - 265s 36ms/step - loss: 2.0661 - perplexity: 7.8939 - accuracy: 0.7224\n",
      "Epoch 3/25\n",
      "7407/7407 [==============================] - 267s 36ms/step - loss: 2.0663 - perplexity: 7.8957 - accuracy: 0.7224\n",
      "Epoch 4/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0667 - perplexity: 7.8985 - accuracy: 0.7224\n",
      "Epoch 5/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0663 - perplexity: 7.8957 - accuracy: 0.7223\n",
      "Epoch 6/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0665 - perplexity: 7.8968 - accuracy: 0.7225\n",
      "Epoch 7/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0668 - perplexity: 7.8992 - accuracy: 0.7224\n",
      "Epoch 8/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0665 - perplexity: 7.8972 - accuracy: 0.7224\n",
      "Epoch 9/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0665 - perplexity: 7.8973 - accuracy: 0.7225\n",
      "Epoch 10/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0664 - perplexity: 7.8966 - accuracy: 0.7224\n",
      "Epoch 11/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0667 - perplexity: 7.8983 - accuracy: 0.7224\n",
      "Epoch 12/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0667 - perplexity: 7.8990 - accuracy: 0.7225\n",
      "Epoch 13/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0667 - perplexity: 7.8990 - accuracy: 0.7225\n",
      "Epoch 14/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0668 - perplexity: 7.8992 - accuracy: 0.7226\n",
      "Epoch 15/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0671 - perplexity: 7.9020 - accuracy: 0.7226\n",
      "Epoch 16/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0669 - perplexity: 7.9000 - accuracy: 0.7225\n",
      "Epoch 17/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0670 - perplexity: 7.9008 - accuracy: 0.7226\n",
      "Epoch 18/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0670 - perplexity: 7.9014 - accuracy: 0.7226\n",
      "Epoch 19/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0671 - perplexity: 7.9017 - accuracy: 0.7227\n",
      "Epoch 20/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0672 - perplexity: 7.9029 - accuracy: 0.7226\n",
      "Epoch 21/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0670 - perplexity: 7.9011 - accuracy: 0.7227\n",
      "Epoch 22/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0672 - perplexity: 7.9029 - accuracy: 0.7227\n",
      "Epoch 23/25\n",
      "7407/7407 [==============================] - 268s 36ms/step - loss: 2.0672 - perplexity: 7.9031 - accuracy: 0.7228\n",
      "Epoch 24/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0675 - perplexity: 7.9047 - accuracy: 0.7228\n",
      "Epoch 25/25\n",
      "7407/7407 [==============================] - 269s 36ms/step - loss: 2.0673 - perplexity: 7.9031 - accuracy: 0.7227\n"
     ]
    }
   ],
   "source": [
    "history3_fairy_simple = loaded_model_transf_fairy_simple3.fit(train_dataset_fairy_simple, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy_simple3.weights)):\n",
    "    loaded_model_transf_fairy_simple3.weights[i]._handle_name = loaded_model_transf_fairy_simple3.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "for i in range(len(loaded_model_transf_fairy_simple3.optimizer.weights)):\n",
    "    loaded_model_transf_fairy_simple3.optimizer.weights[i]._handle_name = loaded_model_transf_fairy_simple3.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_fairy_simple3.save(\"transformer_fairy_simple3.keras\")\n",
    "loaded_model_transf_fairy_simple4 = tf.keras.models.load_model(\"transformer_fairy_simple3.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami a i ale gdy w gdy to to to kiedy nie a to to ale bardzo nad\n",
      "z się mu za zaś następnie samo daleko nim było miał tak się je podział i ale\n",
      "król tak gdy po a ale a i o król z gdy nie już się po łatwo drodze w rzekł\n",
      "poszedł go w drodze ale ojciec w przed i oczyma na był w na to pewnego a kiedy\n",
      "ale w pewnego po tak tak a tobą przez szczęśliwy pewno czasu to lesie czym łatwo\n",
      "ale król ale a sobie posłał na syna gdy nie kiedy król ale król z w na po\n",
      "pewnego ale pewnego w gdy zaledwie lecz miała i był domu mieście król oddali\n",
      "trzy z jeszcze i się nią z głowę a ale nie z o a z nie gdy nie nie z ale gdy\n",
      "kiedy i i na nich to było miała tego młodzieniec jeszcze nią w to miał nie samo\n",
      "mam\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy_simple-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy_simple.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy_simple = max(lens2_fairy_simple)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy_simple+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy_simple4.predict(test_sequence, verbose=0)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy_simple.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 MB Miniset of the most famous and short children fairytales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reading files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 23345 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences_fairy_mini = []\n",
    "\n",
    "for file in glob.glob(\"Mini bajki/*\"):\n",
    "\n",
    "    try:\n",
    "        #read the file\n",
    "        myfile = open(file,\"r\")\n",
    "        text = myfile.read()\n",
    "        myfile.close()\n",
    "\n",
    "        #lower\n",
    "        text = text.lower()\n",
    "\n",
    "        #split to sentences\n",
    "        text = sent_tokenize(text)\n",
    "        #print(\"file \", file, \" generated \", len(text), \" words\")\n",
    "        \n",
    "        sentences_fairy_mini.extend(text)\n",
    "    except:\n",
    "       continue\n",
    "    \n",
    "print(\"We have\", len(sentences_fairy_mini), \"sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text consists of 347565 words.\n"
     ]
    }
   ],
   "source": [
    "continuous_corpus_fairy_mini = \" \".join(sentences_fairy_mini)\n",
    "print(\"Full text consists of\", len(continuous_corpus_fairy_mini.replace('\\n', ' ').split(' ')), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['z muru zwieszały się pnące rośliny, a wielkie liście łopianu schylały się aż do wody.',\n",
       " 'i było pod nimi cicho i ciemno, jak w cienistym lesie.',\n",
       " 'pod jednym z takich liści młoda kaczka usłała sobie gniazdo i siedziała na jajach.',\n",
       " 'nudziło jej się bardzo, bo żadna z sąsiadek nie miała chęci w tak piękną pogodę rozmawiać z nią o tym, co słychać na świecie.',\n",
       " 'każda wolała pływać po przejrzystej wodzie, pluskać się i osuszać na ciepłym słoneczku, a ona tylko jedna, jak przykuta, siedzi w cieniu na gnieździe.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_fairy_mini[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sentences lengths analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are of length 1 to 126\n"
     ]
    }
   ],
   "source": [
    "lens_fairy_mini = []\n",
    "for sentence in sentences_fairy_mini:\n",
    "  lens_fairy_mini.append(len(sentence.replace('\\n', ' ').split(' ')))\n",
    "\n",
    "print(\"Sentences are of length\", min(lens_fairy_mini), \"to\", max(lens_fairy_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles:\n",
      "0.15 is 4 \n",
      "0.5 is 12 \n",
      "0.8 is 23 \n",
      "0.9 is 29 \n",
      "0.95 is 36\n",
      "Let's remove the sentences longer than 36.\n"
     ]
    }
   ],
   "source": [
    "#quantiles - 90% of sequences consists of at most 29 words, at most 15% is of length 4 or less\n",
    "lens_fairy_mini.sort()\n",
    "print(\"Quantiles:\\n0.15 is\", lens_fairy_mini[int(0.15*len(lens_fairy_mini))],\n",
    " \"\\n0.5 is\", lens_fairy_mini[int(0.5*len(lens_fairy_mini))], \n",
    " \"\\n0.8 is\", lens_fairy_mini[int(0.8*len(lens_fairy_mini))], \n",
    " \"\\n0.9 is\", lens_fairy_mini[int(0.9*len(lens_fairy_mini))],\n",
    " \"\\n0.95 is\", lens_fairy_mini[int(0.95*len(lens_fairy_mini))])\n",
    "print(\"Let's remove the sentences longer than 36.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing long sentences\n",
    "#lowering the letters\n",
    "#removing new line signs\n",
    "\n",
    "sentences_short_fairy_mini = []\n",
    "for sentence in sentences_fairy_mini:\n",
    "  if not len(sentence.replace('\\n', ' ').split(' ')) > 36:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('\\n', ' ')\n",
    "    sentence = sentence.replace('—', '-')\n",
    "    sentences_short_fairy_mini.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short sentences are of length 1 to 36\n"
     ]
    }
   ],
   "source": [
    "lens2_fairy_mini = []\n",
    "for sentence in sentences_short_fairy_mini:\n",
    "  lens2_fairy_mini.append(len(sentence.replace('\\n', ' ').split(' ')))\n",
    "\n",
    "print(\"Short sentences are of length\", min(lens2_fairy_mini), \"to\", max(lens2_fairy_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22250"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_short_fairy_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenization. No punctuation included**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No words excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words: 38377\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Tokenizer on the Corpus\n",
    "tokenizer_fairy_mini = Tokenizer(num_words=38377)                   #no words excluded\n",
    "tokenizer_fairy_mini.fit_on_texts(sentences_short_fairy_mini)\n",
    "\n",
    "# Vocabulary count of the corpus\n",
    "total_words_fairy_mini = len(tokenizer_fairy_mini.word_index)\n",
    "\n",
    "print(\"Total Unique Words:\", total_words_fairy_mini)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the text into embeddings\n",
    "input_sequences_fairy_mini = []\n",
    "for sentence in sentences_short_fairy_mini:\n",
    "    token_list = tokenizer_fairy_mini.texts_to_sequences([sentence])[0]\n",
    "    input_sequences_fairy_mini.append(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_fairy_mini = max(lens2_fairy_mini)\n",
    "input_sequences_fairy_mini = np.array(pad_sequences(input_sequences_fairy_mini, maxlen=maxlen_fairy_mini+1, padding='pre'))  #maxlen +1\n",
    "maxlen_fairy_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tensorflow Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset_fairy_mini = tf.data.Dataset.from_tensor_slices(input_sequences_fairy_mini)\n",
    "train_dataset_fairy_mini = train_dataset_fairy_mini.shuffle(buffer_size=256)\n",
    "train_dataset_fairy_mini = train_dataset_fairy_mini.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    print(text.shape)\n",
    "    predictors, labels = text[:, :-1], text[:, 1:]    #offset by one + label is long!\n",
    "    print(predictors.shape, labels.shape)\n",
    "    return predictors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 37, 1)\n",
      "(None, 36, 1) (None, 36, 1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_fairy_mini = train_dataset_fairy_mini.map(preprocessing)\n",
    "train_dataset_fairy_mini = train_dataset_fairy_mini.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same model as above (more heads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transf_input (InputLayer)   [(None, 36)]              0         \n",
      "                                                                 \n",
      " transf_embed (TokenAndPosit  (None, 36, 32)           1229216   \n",
      " ionEmbedding)                                                   \n",
      "                                                                 \n",
      " transf_decod (TransformerDe  (None, 36, 32)           6464      \n",
      " coder)                                                          \n",
      "                                                                 \n",
      " transf_dense (Dense)        (None, 36, 38377)         1266441   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,502,121\n",
      "Trainable params: 2,502,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  #inicially 128\n",
    "num_heads = 8\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(maxlen_fairy_mini, ), dtype=tf.int32, name='transf_input')\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(total_words_fairy_mini, maxlen_fairy_mini, embed_dim, name='transf_embed')(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5, \n",
    "                                                            name='transf_decod')(embedding_layer)\n",
    "    outputs = keras.layers.Dense(total_words_fairy_mini, activation='softmax', name='transf_dense')(decoder)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_transf_fairy_mini = create_model()\n",
    "model_transf_fairy_mini.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More epochs on a small dataset with less words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2782/2782 [==============================] - 94s 32ms/step - loss: 5.2059 - perplexity: 182.3462 - accuracy: 0.6251\n",
      "Epoch 2/50\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 3.0264 - perplexity: 20.6231 - accuracy: 0.6473\n",
      "Epoch 3/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.9433 - perplexity: 18.9782 - accuracy: 0.6538\n",
      "Epoch 4/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 2.8819 - perplexity: 17.8489 - accuracy: 0.6563\n",
      "Epoch 5/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 2.8218 - perplexity: 16.8062 - accuracy: 0.6596\n",
      "Epoch 6/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 2.7622 - perplexity: 15.8342 - accuracy: 0.6634\n",
      "Epoch 7/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.7036 - perplexity: 14.9339 - accuracy: 0.6672\n",
      "Epoch 8/50\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 2.6470 - perplexity: 14.1110 - accuracy: 0.6708\n",
      "Epoch 9/50\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 2.5921 - perplexity: 13.3575 - accuracy: 0.6739\n",
      "Epoch 10/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.5385 - perplexity: 12.6613 - accuracy: 0.6772\n",
      "Epoch 11/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 2.4861 - perplexity: 12.0146 - accuracy: 0.6805\n",
      "Epoch 12/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.4347 - perplexity: 11.4121 - accuracy: 0.6837\n",
      "Epoch 13/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.3848 - perplexity: 10.8565 - accuracy: 0.6869\n",
      "Epoch 14/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.3365 - perplexity: 10.3451 - accuracy: 0.6903\n",
      "Epoch 15/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.2903 - perplexity: 9.8777 - accuracy: 0.6937\n",
      "Epoch 16/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.2462 - perplexity: 9.4519 - accuracy: 0.6970\n",
      "Epoch 17/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.2045 - perplexity: 9.0654 - accuracy: 0.7004\n",
      "Epoch 18/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 2.1654 - perplexity: 8.7179 - accuracy: 0.7037\n",
      "Epoch 19/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 2.1283 - perplexity: 8.4008 - accuracy: 0.7068\n",
      "Epoch 20/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 2.0935 - perplexity: 8.1134 - accuracy: 0.7098\n",
      "Epoch 21/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 2.0609 - perplexity: 7.8531 - accuracy: 0.7124\n",
      "Epoch 22/50\n",
      "2782/2782 [==============================] - 92s 33ms/step - loss: 2.0306 - perplexity: 7.6188 - accuracy: 0.7146\n",
      "Epoch 23/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 2.0023 - perplexity: 7.4061 - accuracy: 0.7165\n",
      "Epoch 24/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.9761 - perplexity: 7.2145 - accuracy: 0.7182\n",
      "Epoch 25/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.9515 - perplexity: 7.0393 - accuracy: 0.7195\n",
      "Epoch 26/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.9286 - perplexity: 6.8799 - accuracy: 0.7206\n",
      "Epoch 27/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.9071 - perplexity: 6.7336 - accuracy: 0.7219\n",
      "Epoch 28/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.8872 - perplexity: 6.6008 - accuracy: 0.7230\n",
      "Epoch 29/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.8687 - perplexity: 6.4797 - accuracy: 0.7240\n",
      "Epoch 30/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.8517 - perplexity: 6.3706 - accuracy: 0.7249\n",
      "Epoch 31/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.8355 - perplexity: 6.2682 - accuracy: 0.7258\n",
      "Epoch 32/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.8209 - perplexity: 6.1774 - accuracy: 0.7265\n",
      "Epoch 33/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.8070 - perplexity: 6.0920 - accuracy: 0.7274\n",
      "Epoch 34/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7938 - perplexity: 6.0121 - accuracy: 0.7282\n",
      "Epoch 35/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7823 - perplexity: 5.9433 - accuracy: 0.7286\n",
      "Epoch 36/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7713 - perplexity: 5.8785 - accuracy: 0.7296\n",
      "Epoch 37/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7605 - perplexity: 5.8154 - accuracy: 0.7303\n",
      "Epoch 38/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7511 - perplexity: 5.7607 - accuracy: 0.7309\n",
      "Epoch 39/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7421 - perplexity: 5.7095 - accuracy: 0.7314\n",
      "Epoch 40/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7334 - perplexity: 5.6601 - accuracy: 0.7321\n",
      "Epoch 41/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7262 - perplexity: 5.6190 - accuracy: 0.7325\n",
      "Epoch 42/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7190 - perplexity: 5.5787 - accuracy: 0.7331\n",
      "Epoch 43/50\n",
      "2782/2782 [==============================] - 90s 33ms/step - loss: 1.7123 - perplexity: 5.5415 - accuracy: 0.7334\n",
      "Epoch 44/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7058 - perplexity: 5.5060 - accuracy: 0.7340\n",
      "Epoch 45/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.7000 - perplexity: 5.4742 - accuracy: 0.7342\n",
      "Epoch 46/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.6946 - perplexity: 5.4446 - accuracy: 0.7346\n",
      "Epoch 47/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.6892 - perplexity: 5.4150 - accuracy: 0.7351\n",
      "Epoch 48/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.6852 - perplexity: 5.3934 - accuracy: 0.7352\n",
      "Epoch 49/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.6801 - perplexity: 5.3663 - accuracy: 0.7356\n",
      "Epoch 50/50\n",
      "2782/2782 [==============================] - 90s 33ms/step - loss: 1.6756 - perplexity: 5.3422 - accuracy: 0.7360\n"
     ]
    }
   ],
   "source": [
    "history_fairy_mini = model_transf_fairy_mini.fit(train_dataset_fairy_mini, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_transf_fairy_mini.weights)):\n",
    "    model_transf_fairy_mini.weights[i]._handle_name = model_transf_fairy_mini.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "for i in range(len(model_transf_fairy_mini.optimizer.weights)):\n",
    "    model_transf_fairy_mini.optimizer.weights[i]._handle_name = model_transf_fairy_mini.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_transf_fairy_mini.save(\"transformer_fairy_mini.keras\")\n",
    "model_transf_fairy_mini2 = tf.keras.models.load_model(\"transformer_fairy_mini.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "    #print(\"logits shape: \", logits.shape)\n",
    "    logits, indices = tf.math.top_k(logits, k=15, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami król król nie nagle a po nie i następnie w nie po\n",
      "młodzieniec i ale chwilę sobą powierzył będziesz jednak jestem następnie ma\n",
      "który ginę do miała córką przed się jeszcze słońcem człowieka kiedy to kiedy\n",
      "potem po kandyd a nie następnie król na młodzieniec kiedy i a jej ani nagle\n",
      "ujrzał skończonej teraz po stole już złotopiórcia śladu las przecież chociaż i\n",
      "otoczony na odjechał to nie wieczorem młodzieniec król król w w nie dnia o\n",
      "młodzieniec klara ale gdy opowiedział chwilę że dotarł przyjął drogę wolno tym\n",
      "zaczęła tymczasem choć przez żeby co graniem chwilę dzień zatrzymywały kiedy\n",
      "klara gdy a kiedy nie gdy w kiedy dnia ale w w nagle następnie dziewicę cały\n",
      "znów ujrzał się spostrzegł za wciąż mgnieniu książęta był ogniste więc pilnowany\n",
      "dobrych domy w stopy rozdział król pewnego kiedy ale nie na nie po dnia pewnego\n",
      "po na z ale które końcu xiii młodzieniec król to cichutku ranka widok odgadła\n",
      "jednak który ty szeroką nosi z śmiertelnym powrotem\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy_mini-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy_mini.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy_mini = max(lens2_fairy_mini)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy_mini+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = model_transf_fairy_mini2.predict(test_sequence, verbose=0)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy_mini.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **More training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy_mini2 = tf.keras.models.load_model(\"transformer_fairy_mini.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2782/2782 [==============================] - 90s 32ms/step - loss: 1.6719 - perplexity: 5.3224 - accuracy: 0.7362\n",
      "Epoch 2/30\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.6680 - perplexity: 5.3016 - accuracy: 0.7365\n",
      "Epoch 3/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6645 - perplexity: 5.2832 - accuracy: 0.7365\n",
      "Epoch 4/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6610 - perplexity: 5.2647 - accuracy: 0.7369\n",
      "Epoch 5/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6576 - perplexity: 5.2465 - accuracy: 0.7372\n",
      "Epoch 6/30\n",
      "2782/2782 [==============================] - 86s 31ms/step - loss: 1.6548 - perplexity: 5.2320 - accuracy: 0.7372\n",
      "Epoch 7/30\n",
      "2782/2782 [==============================] - 88s 31ms/step - loss: 1.6514 - perplexity: 5.2144 - accuracy: 0.7377\n",
      "Epoch 8/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6485 - perplexity: 5.1991 - accuracy: 0.7378\n",
      "Epoch 9/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6460 - perplexity: 5.1862 - accuracy: 0.7380\n",
      "Epoch 10/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6436 - perplexity: 5.1736 - accuracy: 0.7382\n",
      "Epoch 11/30\n",
      "2782/2782 [==============================] - 87s 31ms/step - loss: 1.6412 - perplexity: 5.1616 - accuracy: 0.7382\n",
      "Epoch 12/30\n",
      "2782/2782 [==============================] - 87s 31ms/step - loss: 1.6383 - perplexity: 5.1465 - accuracy: 0.7385\n",
      "Epoch 13/30\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.6359 - perplexity: 5.1340 - accuracy: 0.7389\n",
      "Epoch 14/30\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.6336 - perplexity: 5.1222 - accuracy: 0.7390\n",
      "Epoch 15/30\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.6319 - perplexity: 5.1134 - accuracy: 0.7391\n",
      "Epoch 16/30\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.6303 - perplexity: 5.1053 - accuracy: 0.7391\n",
      "Epoch 17/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6281 - perplexity: 5.0944 - accuracy: 0.7394\n",
      "Epoch 18/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6262 - perplexity: 5.0845 - accuracy: 0.7394\n",
      "Epoch 19/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6242 - perplexity: 5.0745 - accuracy: 0.7397\n",
      "Epoch 20/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6222 - perplexity: 5.0644 - accuracy: 0.7398\n",
      "Epoch 21/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6209 - perplexity: 5.0579 - accuracy: 0.7398\n",
      "Epoch 22/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6194 - perplexity: 5.0499 - accuracy: 0.7401\n",
      "Epoch 23/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6175 - perplexity: 5.0405 - accuracy: 0.7401\n",
      "Epoch 24/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6163 - perplexity: 5.0343 - accuracy: 0.7403\n",
      "Epoch 25/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6145 - perplexity: 5.0256 - accuracy: 0.7404\n",
      "Epoch 26/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6127 - perplexity: 5.0163 - accuracy: 0.7405\n",
      "Epoch 27/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6117 - perplexity: 5.0112 - accuracy: 0.7407\n",
      "Epoch 28/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6102 - perplexity: 5.0040 - accuracy: 0.7409\n",
      "Epoch 29/30\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.6089 - perplexity: 4.9975 - accuracy: 0.7411\n",
      "Epoch 30/30\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.6080 - perplexity: 4.9929 - accuracy: 0.7409\n"
     ]
    }
   ],
   "source": [
    "history2_fairy_mini = loaded_model_transf_fairy_mini2.fit(train_dataset_fairy_mini, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy_mini2.weights)):\n",
    "    loaded_model_transf_fairy_mini2.weights[i]._handle_name = loaded_model_transf_fairy_mini2.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "for i in range(len(loaded_model_transf_fairy_mini2.optimizer.weights)):\n",
    "    loaded_model_transf_fairy_mini2.optimizer.weights[i]._handle_name = loaded_model_transf_fairy_mini2.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model_transf_fairy_mini2.save(\"transformer_fairy_mini2.keras\")\n",
    "loaded_model_transf_fairy_mini3 = tf.keras.models.load_model(\"transformer_fairy_mini2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami po kiedy gdy król i a o na i młodzieniec kiedy a król z na\n",
      "chwilę duże rowach nie nie mało zwieszała znów po pogawędki a zjem z dotarł\n",
      "rozpostarte tak tam drzesz kiedy gdy gdy po na gdy o i na dnia następnego\n",
      "młodzieniec po i na za do byli do klombach tym łące księcia chwili poddasze\n",
      "tańczącego nich nie klaro także jak rzekł zaczarowane kiedy na to a tak kiedy\n",
      "pewnego gdy po a kiedy i pewnego gdy król o w nagle że się przyszli czym\n",
      "doralice wieczoru spostrzegł dalszą z w kawalerem to niej przywitała pazurami a\n",
      "gdy więc wieczorem a w o kiedy król nie ale na o a król był jej wiatr płacili w\n",
      "tkaczach udał już swoim myszy mowę… dla nie za polnej gościa mną serdecznik na\n",
      "klara to w król w w kiedy a i po król król nie nie jest gdyż podwórzu nie kazał\n",
      "środku młodzieniec czym nie mogła nie chce tryskała księżniczce udać je czytać\n",
      "sprzedać\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy_mini-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy_mini.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy_mini = max(lens2_fairy_mini)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy_mini+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy_mini3.predict(test_sequence, verbose=0)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy_mini.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Even more training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy_mini3 = tf.keras.models.load_model(\"transformer_fairy_mini2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 1.6062 - perplexity: 4.9838 - accuracy: 0.7412\n",
      "Epoch 2/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 1.6055 - perplexity: 4.9805 - accuracy: 0.7413\n",
      "Epoch 3/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 1.6039 - perplexity: 4.9726 - accuracy: 0.7413\n",
      "Epoch 4/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 1.6023 - perplexity: 4.9643 - accuracy: 0.7413\n",
      "Epoch 5/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.6016 - perplexity: 4.9609 - accuracy: 0.7416\n",
      "Epoch 6/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.6007 - perplexity: 4.9565 - accuracy: 0.7417\n",
      "Epoch 7/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5993 - perplexity: 4.9497 - accuracy: 0.7418\n",
      "Epoch 8/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5987 - perplexity: 4.9468 - accuracy: 0.7417\n",
      "Epoch 9/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5975 - perplexity: 4.9408 - accuracy: 0.7419\n",
      "Epoch 10/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5962 - perplexity: 4.9344 - accuracy: 0.7422\n",
      "Epoch 11/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.5953 - perplexity: 4.9297 - accuracy: 0.7420\n",
      "Epoch 12/50\n",
      "2782/2782 [==============================] - 92s 33ms/step - loss: 1.5942 - perplexity: 4.9242 - accuracy: 0.7425\n",
      "Epoch 13/50\n",
      "2782/2782 [==============================] - 91s 33ms/step - loss: 1.5931 - perplexity: 4.9192 - accuracy: 0.7425\n",
      "Epoch 14/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5925 - perplexity: 4.9160 - accuracy: 0.7424\n",
      "Epoch 15/50\n",
      "2782/2782 [==============================] - 87s 31ms/step - loss: 1.5918 - perplexity: 4.9126 - accuracy: 0.7427\n",
      "Epoch 16/50\n",
      "2782/2782 [==============================] - 87s 31ms/step - loss: 1.5907 - perplexity: 4.9070 - accuracy: 0.7428\n",
      "Epoch 17/50\n",
      "2782/2782 [==============================] - 88s 31ms/step - loss: 1.5897 - perplexity: 4.9024 - accuracy: 0.7425\n",
      "Epoch 18/50\n",
      "2782/2782 [==============================] - 88s 31ms/step - loss: 1.5892 - perplexity: 4.8996 - accuracy: 0.7429\n",
      "Epoch 19/50\n",
      "2782/2782 [==============================] - 88s 31ms/step - loss: 1.5885 - perplexity: 4.8965 - accuracy: 0.7430\n",
      "Epoch 20/50\n",
      "2782/2782 [==============================] - 88s 31ms/step - loss: 1.5872 - perplexity: 4.8902 - accuracy: 0.7432\n",
      "Epoch 21/50\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.5864 - perplexity: 4.8862 - accuracy: 0.7430\n",
      "Epoch 22/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5864 - perplexity: 4.8860 - accuracy: 0.7430\n",
      "Epoch 23/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5852 - perplexity: 4.8804 - accuracy: 0.7435\n",
      "Epoch 24/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5839 - perplexity: 4.8741 - accuracy: 0.7434\n",
      "Epoch 25/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5834 - perplexity: 4.8714 - accuracy: 0.7434\n",
      "Epoch 26/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5830 - perplexity: 4.8696 - accuracy: 0.7434\n",
      "Epoch 27/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5820 - perplexity: 4.8648 - accuracy: 0.7435\n",
      "Epoch 28/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5810 - perplexity: 4.8600 - accuracy: 0.7435\n",
      "Epoch 29/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5808 - perplexity: 4.8588 - accuracy: 0.7436\n",
      "Epoch 30/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5798 - perplexity: 4.8541 - accuracy: 0.7438\n",
      "Epoch 31/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5790 - perplexity: 4.8500 - accuracy: 0.7438\n",
      "Epoch 32/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5786 - perplexity: 4.8484 - accuracy: 0.7439\n",
      "Epoch 33/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5782 - perplexity: 4.8463 - accuracy: 0.7439\n",
      "Epoch 34/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5774 - perplexity: 4.8425 - accuracy: 0.7442\n",
      "Epoch 35/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5764 - perplexity: 4.8373 - accuracy: 0.7444\n",
      "Epoch 36/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5766 - perplexity: 4.8384 - accuracy: 0.7441\n",
      "Epoch 37/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5757 - perplexity: 4.8340 - accuracy: 0.7443\n",
      "Epoch 38/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5753 - perplexity: 4.8323 - accuracy: 0.7443\n",
      "Epoch 39/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5743 - perplexity: 4.8274 - accuracy: 0.7444\n",
      "Epoch 40/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5743 - perplexity: 4.8273 - accuracy: 0.7444\n",
      "Epoch 41/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5737 - perplexity: 4.8245 - accuracy: 0.7442\n",
      "Epoch 42/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5733 - perplexity: 4.8226 - accuracy: 0.7443\n",
      "Epoch 43/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5720 - perplexity: 4.8160 - accuracy: 0.7446\n",
      "Epoch 44/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5711 - perplexity: 4.8118 - accuracy: 0.7448\n",
      "Epoch 45/50\n",
      "2782/2782 [==============================] - 90s 32ms/step - loss: 1.5713 - perplexity: 4.8131 - accuracy: 0.7443\n",
      "Epoch 46/50\n",
      "2782/2782 [==============================] - 89s 32ms/step - loss: 1.5708 - perplexity: 4.8103 - accuracy: 0.7446\n",
      "Epoch 47/50\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.5704 - perplexity: 4.8084 - accuracy: 0.7447\n",
      "Epoch 48/50\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.5693 - perplexity: 4.8034 - accuracy: 0.7450\n",
      "Epoch 49/50\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.5700 - perplexity: 4.8067 - accuracy: 0.7449\n",
      "Epoch 50/50\n",
      "2782/2782 [==============================] - 88s 32ms/step - loss: 1.5688 - perplexity: 4.8010 - accuracy: 0.7449\n"
     ]
    }
   ],
   "source": [
    "history3_fairy_mini = loaded_model_transf_fairy_mini3.fit(train_dataset_fairy_mini, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loaded_model_transf_fairy_mini3.weights)):\n",
    "    loaded_model_transf_fairy_mini3.weights[i]._handle_name = loaded_model_transf_fairy_mini3.weights[i].name + \"_\" + str(i)\n",
    "\n",
    "for i in range(len(loaded_model_transf_fairy_mini3.optimizer.weights)):\n",
    "    loaded_model_transf_fairy_mini3.optimizer.weights[i]._handle_name = loaded_model_transf_fairy_mini3.optimizer.weights[i].name + \"_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_transf_fairy_mini3.save(\"transformer_fairy_mini3.keras\")\n",
    "loaded_model_transf_fairy_mini4 = tf.keras.models.load_model(\"transformer_fairy_mini3.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Za górami za lasami po to i i i ale o a po na kiedy w to o po mąż pierwszym jej\n",
      "uciekł z pień cichutku ujrzała przed horyzont gdyż ile twarzy i i by kacze\n",
      "spalono młodzieniec nie nie wieczorem o gdy król pewnego na kiedy gdy na w król\n",
      "a opowiedział podwórze rzucił będziesz litość posłał myśl tylko kołysce macocha\n",
      "i w pierścień z jest swoim mojego przyjacielem książę więc w to tak to nie z\n",
      "złotopiórcia kiedy a kiedy w na ale bardzo państwa nakazał końcu już troszczyła\n",
      "piękna co końcu nie mojego na mi to chce to go wierzyć a na pewnego wieczorem\n",
      "nie a gdy a król to a kiedy kiedy w następnie znowu nie gdy wieczora był król\n",
      "pojął gdy na torbę ma tropił i kukułka kotarami córeczkę zetną skrzynkę klara\n",
      "ale i nie a i to po nie nie o po a a po który przez po z jednak tylko wolno\n",
      "świcie co dotarciu niego rąk co młodzieniec przy oddali zrobił dzieci\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Za górami za lasami'\n",
    "full_text = 'Za górami za lasami'\n",
    "\n",
    "#preprocessing\n",
    "seed_text = seed_text.lower()\n",
    "seed_text = seed_text.replace('\\n', ' ')\n",
    "seed_text = seed_text.replace('—', '-')\n",
    "\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences): #correct approach? otherwise error if sample_index is 40 or more\n",
    "\n",
    "    sample_index = 0\n",
    "    #one sentence length\n",
    "    while maxlen_fairy_mini-1 > sample_index:\n",
    "\n",
    "        #embeddings\n",
    "        token_list = tokenizer_fairy_mini.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #padding\n",
    "        maxlen_fairy_mini = max(lens2_fairy_mini)\n",
    "        test_sequence = np.array(pad_sequences([token_list], maxlen=maxlen_fairy_mini+1, padding='pre'))\n",
    "\n",
    "        #test sample\n",
    "        test_sequence = test_sequence[:, :-1]\n",
    "\n",
    "        #predictions\n",
    "        soft_pred = loaded_model_transf_fairy_mini4.predict(test_sequence, verbose=0)\n",
    "\n",
    "        sample_index = len(seed_text.strip().split())-1\n",
    "        sampled_token = sample_token(soft_pred[0][sample_index])\n",
    "\n",
    "        output_word = \"\"\n",
    "        #decoding tokens\n",
    "        for word, index in tokenizer_fairy_mini.word_index.items():\n",
    "            if index == sampled_token:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    #save text generated so far\n",
    "    full_text += ' '\n",
    "    full_text += ' '.join(seed_text.split()[4:])\n",
    "    #reset seed_text (set as current last 4 words)\n",
    "    seed_text =  ' '.join(seed_text.split()[-4:])\n",
    "\n",
    "print('\\n'.join(textwrap.wrap(full_text, 80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
